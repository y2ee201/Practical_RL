{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning with crossentropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DISPLAY=:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'bash' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-27 10:52:21,639] Making new env: Taxi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "from gym import wrappers\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "# env = wrappers.Monitor(env,'./monitor')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states,n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.ones((n_states, n_actions))/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray,np.matrix)\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        a = int(np.random.choice(n_actions, 1, p = policy[s]))\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward = total_reward + r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,a,r = generate_session()\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) is float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.87 s\n",
      "mean reward = -771.83200\tthreshold = -780.5\n",
      "Wall time: 3.07 s\n",
      "mean reward = -695.98000\tthreshold = -713.0\n",
      "Wall time: 2.85 s\n",
      "mean reward = -611.29200\tthreshold = -650.0\n",
      "Wall time: 2.5 s\n",
      "mean reward = -526.08800\tthreshold = -578.5\n",
      "Wall time: 1.85 s\n",
      "mean reward = -406.54400\tthreshold = -450.0\n",
      "Wall time: 1.41 s\n",
      "mean reward = -278.80000\tthreshold = -252.5\n",
      "Wall time: 906 ms\n",
      "mean reward = -161.74800\tthreshold = -140.5\n",
      "Wall time: 656 ms\n",
      "mean reward = -100.98400\tthreshold = -85.5\n",
      "Wall time: 578 ms\n",
      "mean reward = -71.56000\tthreshold = -51.0\n",
      "Wall time: 535 ms\n",
      "mean reward = -44.05200\tthreshold = -34.5\n",
      "Wall time: 661 ms\n",
      "mean reward = -37.34400\tthreshold = -19.0\n",
      "Wall time: 422 ms\n",
      "mean reward = -32.92800\tthreshold = -14.0\n",
      "Wall time: 422 ms\n",
      "mean reward = -31.47200\tthreshold = -6.0\n",
      "Wall time: 344 ms\n",
      "mean reward = -24.54400\tthreshold = 0.0\n",
      "Wall time: 359 ms\n",
      "mean reward = -27.82800\tthreshold = 2.0\n",
      "Wall time: 422 ms\n",
      "mean reward = -39.27600\tthreshold = 3.0\n",
      "Wall time: 516 ms\n",
      "mean reward = -66.22000\tthreshold = -1.0\n",
      "Wall time: 578 ms\n",
      "mean reward = -62.38000\tthreshold = 2.0\n",
      "Wall time: 641 ms\n",
      "mean reward = -91.31200\tthreshold = 1.5\n",
      "Wall time: 641 ms\n",
      "mean reward = -66.66000\tthreshold = 0.0\n",
      "Wall time: 520 ms\n",
      "mean reward = -78.60400\tthreshold = -1.5\n",
      "Wall time: 902 ms\n",
      "mean reward = -63.61600\tthreshold = 3.5\n",
      "Wall time: 641 ms\n",
      "mean reward = -94.53200\tthreshold = -4.5\n",
      "Wall time: 531 ms\n",
      "mean reward = -80.21600\tthreshold = 1.0\n",
      "Wall time: 563 ms\n",
      "mean reward = -91.78800\tthreshold = 2.0\n",
      "Wall time: 578 ms\n",
      "mean reward = -99.08800\tthreshold = 3.0\n",
      "Wall time: 828 ms\n",
      "mean reward = -142.13600\tthreshold = -1.0\n",
      "Wall time: 844 ms\n",
      "mean reward = -163.11600\tthreshold = 3.0\n",
      "Wall time: 691 ms\n",
      "mean reward = -126.98400\tthreshold = 0.0\n",
      "Wall time: 703 ms\n",
      "mean reward = -119.42000\tthreshold = 4.0\n",
      "Wall time: 750 ms\n",
      "mean reward = -135.46000\tthreshold = 4.0\n",
      "Wall time: 703 ms\n",
      "mean reward = -136.71600\tthreshold = 0.0\n",
      "Wall time: 813 ms\n",
      "mean reward = -119.03200\tthreshold = 5.0\n",
      "Wall time: 813 ms\n",
      "mean reward = -155.60800\tthreshold = -5.5\n",
      "Wall time: 567 ms\n",
      "mean reward = -116.43200\tthreshold = 1.0\n",
      "Wall time: 577 ms\n",
      "mean reward = -118.03600\tthreshold = 3.5\n",
      "Wall time: 578 ms\n",
      "mean reward = -121.36000\tthreshold = 4.0\n",
      "Wall time: 594 ms\n",
      "mean reward = -125.03200\tthreshold = -1.0\n",
      "Wall time: 672 ms\n",
      "mean reward = -119.82800\tthreshold = -1.5\n",
      "Wall time: 688 ms\n",
      "mean reward = -139.63200\tthreshold = -0.5\n",
      "Wall time: 625 ms\n",
      "mean reward = -94.20400\tthreshold = 3.0\n",
      "Wall time: 516 ms\n",
      "mean reward = -79.04000\tthreshold = 4.5\n",
      "Wall time: 453 ms\n",
      "mean reward = -69.56800\tthreshold = 6.0\n",
      "Wall time: 551 ms\n",
      "mean reward = -103.49600\tthreshold = -11.0\n",
      "Wall time: 500 ms\n",
      "mean reward = -97.28000\tthreshold = 1.0\n",
      "Wall time: 438 ms\n",
      "mean reward = -80.30800\tthreshold = 5.0\n",
      "Wall time: 484 ms\n",
      "mean reward = -91.29200\tthreshold = -3.5\n",
      "Wall time: 469 ms\n",
      "mean reward = -86.20000\tthreshold = 2.0\n",
      "Wall time: 625 ms\n",
      "mean reward = -111.48400\tthreshold = 3.0\n",
      "Wall time: 516 ms\n",
      "mean reward = -97.78400\tthreshold = 6.0\n",
      "Wall time: 594 ms\n",
      "mean reward = -136.40800\tthreshold = -14.0\n",
      "Wall time: 567 ms\n",
      "mean reward = -124.21600\tthreshold = -1.5\n",
      "Wall time: 473 ms\n",
      "mean reward = -90.44000\tthreshold = 4.0\n",
      "Wall time: 469 ms\n",
      "mean reward = -89.15600\tthreshold = 4.5\n",
      "Wall time: 563 ms\n",
      "mean reward = -128.96000\tthreshold = 3.0\n",
      "Wall time: 766 ms\n",
      "mean reward = -162.65600\tthreshold = -4.0\n",
      "Wall time: 578 ms\n",
      "mean reward = -121.06400\tthreshold = 3.0\n",
      "Wall time: 578 ms\n",
      "mean reward = -116.64000\tthreshold = -2.5\n",
      "Wall time: 594 ms\n",
      "mean reward = -122.97600\tthreshold = 2.0\n",
      "Wall time: 453 ms\n",
      "mean reward = -80.04000\tthreshold = 6.0\n",
      "Wall time: 563 ms\n",
      "mean reward = -108.87200\tthreshold = -13.0\n",
      "Wall time: 489 ms\n",
      "mean reward = -89.18800\tthreshold = -1.0\n",
      "Wall time: 499 ms\n",
      "mean reward = -94.44800\tthreshold = 4.0\n",
      "Wall time: 578 ms\n",
      "mean reward = -105.67200\tthreshold = 4.0\n",
      "Wall time: 453 ms\n",
      "mean reward = -82.52800\tthreshold = 6.0\n",
      "Wall time: 563 ms\n",
      "mean reward = -118.52400\tthreshold = -12.5\n",
      "Wall time: 547 ms\n",
      "mean reward = -115.41600\tthreshold = 1.0\n",
      "Wall time: 438 ms\n",
      "mean reward = -71.69600\tthreshold = 5.0\n",
      "Wall time: 547 ms\n",
      "mean reward = -98.27600\tthreshold = -2.5\n",
      "Wall time: 484 ms\n",
      "mean reward = -83.84800\tthreshold = 3.5\n",
      "Wall time: 500 ms\n",
      "mean reward = -95.40800\tthreshold = 2.0\n",
      "Wall time: 442 ms\n",
      "mean reward = -73.53200\tthreshold = 4.5\n",
      "Wall time: 484 ms\n",
      "mean reward = -94.04000\tthreshold = 5.0\n",
      "Wall time: 500 ms\n",
      "mean reward = -77.37600\tthreshold = 6.0\n",
      "Wall time: 516 ms\n",
      "mean reward = -100.82800\tthreshold = -4.0\n",
      "Wall time: 422 ms\n",
      "mean reward = -72.10800\tthreshold = 7.0\n",
      "Wall time: 719 ms\n",
      "mean reward = -165.46800\tthreshold = -15.0\n",
      "Wall time: 844 ms\n",
      "mean reward = -169.06000\tthreshold = -8.5\n",
      "Wall time: 781 ms\n",
      "mean reward = -149.15200\tthreshold = -1.5\n",
      "Wall time: 739 ms\n",
      "mean reward = -163.96000\tthreshold = 1.5\n",
      "Wall time: 601 ms\n",
      "mean reward = -140.65600\tthreshold = 4.0\n",
      "Wall time: 703 ms\n",
      "mean reward = -163.42800\tthreshold = -6.0\n",
      "Wall time: 609 ms\n",
      "mean reward = -146.92800\tthreshold = 3.5\n",
      "Wall time: 672 ms\n",
      "mean reward = -162.19200\tthreshold = -1.5\n",
      "Wall time: 547 ms\n",
      "mean reward = -121.39600\tthreshold = 6.0\n",
      "Wall time: 594 ms\n",
      "mean reward = -137.27200\tthreshold = -4.5\n",
      "Wall time: 641 ms\n",
      "mean reward = -138.66000\tthreshold = 4.0\n",
      "Wall time: 801 ms\n",
      "mean reward = -154.66400\tthreshold = 3.0\n",
      "Wall time: 766 ms\n",
      "mean reward = -146.78800\tthreshold = -0.5\n",
      "Wall time: 781 ms\n",
      "mean reward = -160.08800\tthreshold = 5.0\n",
      "Wall time: 813 ms\n",
      "mean reward = -132.57200\tthreshold = -3.0\n",
      "Wall time: 609 ms\n",
      "mean reward = -125.64400\tthreshold = 3.5\n",
      "Wall time: 646 ms\n",
      "mean reward = -115.40800\tthreshold = 4.5\n",
      "Wall time: 598 ms\n",
      "mean reward = -120.64400\tthreshold = 1.5\n",
      "Wall time: 765 ms\n",
      "mean reward = -145.77600\tthreshold = -1.0\n",
      "Wall time: 578 ms\n",
      "mean reward = -118.55600\tthreshold = 4.0\n",
      "Wall time: 766 ms\n",
      "mean reward = -150.74800\tthreshold = -4.0\n",
      "Wall time: 719 ms\n",
      "mean reward = -157.29600\tthreshold = -3.0\n",
      "Wall time: 703 ms\n",
      "mean reward = -140.79200\tthreshold = 4.5\n",
      "Wall time: 688 ms\n",
      "mean reward = -149.30800\tthreshold = 1.0\n"
     ]
    }
   ],
   "source": [
    "n_samples = 250  #sample this many samples\n",
    "percentile = 50  #take this percent of session with highest rewards\n",
    "smoothing = 0.01  #add this thing to all counts for stability\n",
    "\n",
    "reward_list = []\n",
    "threshold_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    %time sessions = [generate_session() for i in range(n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "    \n",
    "    threshold = np.percentile(batch_rewards, percentile)\n",
    "    \n",
    "    index_threshold = [i for i, j in enumerate(batch_rewards) if j > threshold]\n",
    "    \n",
    "    elite_states = batch_states[index_threshold] \n",
    "    elite_actions = batch_actions[index_threshold]\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #hint on task above: use np.percentile and numpy-style indexing\n",
    "    \n",
    "    #count actions from elite states\n",
    "    elite_counts = np.zeros_like(policy)+smoothing\n",
    "    \n",
    "    for i,j in zip(elite_states, elite_actions):\n",
    "        elite_counts[i, j] = elite_counts[i, j] + 1\n",
    "    \n",
    "    for i in range(elite_counts.shape[0]):\n",
    "        elite_counts[i,] = elite_counts[i,]/np.sum(elite_counts[i,])\n",
    "            \n",
    "    policy = elite_counts\n",
    "    \n",
    "    reward_list.append(np.mean(batch_rewards))\n",
    "    threshold_list.append(threshold)\n",
    "    \n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA6WUUmgyUEopBfw/9M5SQlUFIqMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ebe4c2ec88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(reward_list, color='k')\n",
    "plt.plot(threshold_list, color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[42mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "20\n",
      "Done\n",
      "steps: 12, reward:8\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "reward_total = 0\n",
    "for i in range(1000):\n",
    "    env.render()\n",
    "    new_s, reward, done, _ = env.step(np.argmax(policy[s]))\n",
    "    s = new_s\n",
    "    reward_total = reward_total + reward\n",
    "    print(reward)\n",
    "    if done:\n",
    "        print('Done')\n",
    "        break\n",
    "print('steps: {0}, reward:{1}'.format(i, reward_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) crossentropy method\n",
    "\n",
    "In this section we will train a neural network policy for continuous action space game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-27 11:08:16,747] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13f45595828>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEeBJREFUeJzt3V+MXGd5x/Hvr04IiKRN0mwt13YaR3IrOah16MqlAqGU\nCOKmVR1uIiMV+SKVc+EiUCu1DkgFLizRij+9Cqopaa0WcK0CjRXRVo6bCiHRmA04wXZishBHtuXY\nCxRBemFq5+nFnDSDWe/O7ux4mTffjzSac97zZ55HiX579vi8O6kqJEnt+bnlLkCSNBoGvCQ1yoCX\npEYZ8JLUKANekhplwEtSo0YW8Ek2JzmeZDrJzlF9jiRpdhnFc/BJVgDfAt4OnAK+Bryrqo4t+YdJ\nkmY1qiv4TcB0VX2nqn4M7AW2jOizJEmzuGpE510NnOxbPwX81uV2vummm+qWW24ZUSmSNH5OnDjB\nd7/73QxzjlEF/LySbAe2A9x8881MTU0tVymS9DNncnJy6HOM6hbNaWBt3/qabuz/VdXuqpqsqsmJ\niYkRlSFJr16jCvivAeuTrEvyGmArsH9EnyVJmsVIbtFU1YUkfwz8O7ACeKiqjo7isyRJsxvZPfiq\n+hLwpVGdX5I0N2eySlKjDHhJapQBL0mNMuAlqVEGvCQ1yoCXpEYZ8JLUKANekhplwEtSowx4SWqU\nAS9JjTLgJalRBrwkNcqAl6RGGfCS1CgDXpIaZcBLUqMMeElq1FBf2ZfkBPAj4CJwoaomk9wI/BNw\nC3ACuLeq/nu4MiVJC7UUV/C/U1Ubq2qyW98JHKyq9cDBbl2SdIWN4hbNFmBPt7wHuGcEnyFJmsew\nAV/Ao0meSLK9G1tZVWe65ReAlUN+hiRpEYa6Bw+8papOJ/kl4ECSZ/o3VlUlqdkO7H4gbAe4+eab\nhyxDknSpoa7gq+p0934O+CKwCTibZBVA937uMsfurqrJqpqcmJgYpgxJ0iwWHfBJXp/kupeXgXcA\nR4D9wLZut23Aw8MWKUlauGFu0awEvpjk5fN8tqr+LcnXgH1J7gOeB+4dvkxJ0kItOuCr6jvAb8wy\n/j3gzmGKkiQNz5msktQoA16SGmXAS1KjDHhJapQBL0mNMuAlqVEGvCQ1yoCXpEYZ8JLUKANekhpl\nwEtSowx4SWqUAS9JjTLgJalRBrwkNcqAl6RGGfCS1CgDXpIaZcBLUqPmDfgkDyU5l+RI39iNSQ4k\nebZ7v6Fv2wNJppMcT3LXqAqXJM1tkCv4vwc2XzK2EzhYVeuBg906STYAW4HbumMeTLJiyaqVJA1s\n3oCvqi8D379keAuwp1veA9zTN763qs5X1XPANLBpiWqVJC3AYu/Br6yqM93yC8DKbnk1cLJvv1Pd\n2E9Jsj3JVJKpmZmZRZYhSbqcof+RtaoKqEUct7uqJqtqcmJiYtgyJEmXWGzAn02yCqB7P9eNnwbW\n9u23phuTJF1hiw34/cC2bnkb8HDf+NYk1yRZB6wHDg1XoiRpMa6ab4cknwPuAG5Kcgr4IPARYF+S\n+4DngXsBqupokn3AMeACsKOqLo6odknSHOYN+Kp612U23XmZ/XcBu4YpSpI0PGeySlKjDHhJapQB\nL0mNMuAlqVEGvCQ1yoCXpEYZ8JLUKANekhplwEtSowx4SWqUAS9JjTLgJalRBrwkNcqAl6RGGfCS\n1CgDXpIaZcBLUqMMeElq1LwBn+ShJOeSHOkb+1CS00kOd6+7+7Y9kGQ6yfEkd42qcEnS3Aa5gv97\nYPMs45+oqo3d60sASTYAW4HbumMeTLJiqYqVJA1u3oCvqi8D3x/wfFuAvVV1vqqeA6aBTUPUJ0la\npGHuwb8nyVPdLZwburHVwMm+fU51Yz8lyfYkU0mmZmZmhihDkjSbxQb8J4FbgY3AGeBjCz1BVe2u\nqsmqmpyYmFhkGZKky1lUwFfV2aq6WFUvAZ/ildswp4G1fbuu6cYkSVfYogI+yaq+1XcCLz9hsx/Y\nmuSaJOuA9cCh4UqUJC3GVfPtkORzwB3ATUlOAR8E7kiyESjgBHA/QFUdTbIPOAZcAHZU1cXRlC5J\nmsu8AV9V75pl+NNz7L8L2DVMUZKk4TmTVZIaZcBLUqMMeElqlAEvSY0y4CWpUQa8JDVq3sckpVeL\nJ3bf/1Njv7n9b5ahEmlpeAUvSY0y4CWpUQa8JDXKgJekRhnwktQoA16SGmXAS1KjDHhJapQBL0mN\nMuAlqVEGvCQ1at6AT7I2yWNJjiU5muS93fiNSQ4kebZ7v6HvmAeSTCc5nuSuUTYgSZrdIFfwF4A/\nraoNwJuAHUk2ADuBg1W1HjjYrdNt2wrcBmwGHkyyYhTFS5Iub96Ar6ozVfX1bvlHwNPAamALsKfb\nbQ9wT7e8BdhbVeer6jlgGti01IVLkua2oHvwSW4BbgceB1ZW1Zlu0wvAym55NXCy77BT3dil59qe\nZCrJ1MzMzALLliTNZ+CAT3It8HngfVX1w/5tVVVALeSDq2p3VU1W1eTExMRCDpUkDWCggE9yNb1w\n/0xVfaEbPptkVbd9FXCuGz8NrO07fE03Jkm6ggZ5iibAp4Gnq+rjfZv2A9u65W3Aw33jW5Nck2Qd\nsB44tHQlS5IGMchX9r0ZeDfwzSSHu7H3Ax8B9iW5D3geuBegqo4m2Qcco/cEzo6qurjklUuS5jRv\nwFfVV4BcZvOdlzlmF7BriLokSUNyJqskNcqAl6RGGfCS1CgDXpIaZcBLUqMMeElqlAEvSY0y4CWp\nUQa8JDXKgJekRhnwktQoA16SGmXAS1KjDHhJapQBL0mNMuAlqVEGvCQ1yoCXpEYN8qXba5M8luRY\nkqNJ3tuNfyjJ6SSHu9fdfcc8kGQ6yfEkd42yAUnS7Ab50u0LwJ9W1deTXAc8keRAt+0TVfXR/p2T\nbAC2ArcBvww8muRX/eJtSbqy5r2Cr6ozVfX1bvlHwNPA6jkO2QLsrarzVfUcMA1sWopiJUmDW9A9\n+CS3ALcDj3dD70nyVJKHktzQja0GTvYddoq5fyBIkkZg4IBPci3weeB9VfVD4JPArcBG4AzwsYV8\ncJLtSaaSTM3MzCzkUEnSAAYK+CRX0wv3z1TVFwCq6mxVXayql4BP8cptmNPA2r7D13RjP6GqdlfV\nZFVNTkxMDNODJGkWgzxFE+DTwNNV9fG+8VV9u70TONIt7we2JrkmyTpgPXBo6UqWJA1ikKdo3gy8\nG/hmksPd2PuBdyXZCBRwArgfoKqOJtkHHKP3BM4On6CRpCtv3oCvqq8AmWXTl+Y4Zhewa4i6JElD\nciarBDyx+/7lLkFacga8JDXKgJekRhnwktQoA16SGmXAS1KjDHhJapQBL0mNMuAlqVEGvCQ1yoCX\npEYZ8JLUKANekhplwKtZSQZ+jfIc0nIx4CWpUYN84Yf0qvDIme0/sf77q3YvUyXS0vAKXrqMSwNf\nGjcGvIRhrjYN8qXbr01yKMmTSY4m+XA3fmOSA0me7d5v6DvmgSTTSY4nuWuUDUhLwdsxatEgV/Dn\ngbdV1W8AG4HNSd4E7AQOVtV64GC3TpINwFbgNmAz8GCSFaMoXholQ1/jbpAv3S7gxW716u5VwBbg\njm58D/CfwJ9343ur6jzwXJJpYBPw1aUsXFpKk/fvBn4y0D+0LJVIS2ege/BJViQ5DJwDDlTV48DK\nqjrT7fICsLJbXg2c7Dv8VDcmSbqCBgr4qrpYVRuBNcCmJG+4ZHvRu6ofWJLtSaaSTM3MzCzkUEnS\nABb0FE1V/QB4jN699bNJVgF07+e63U4Da/sOW9ONXXqu3VU1WVWTExMTi6ldkjSHQZ6imUhyfbf8\nOuDtwDPAfmBbt9s24OFueT+wNck1SdYB64FDS124JGlug8xkXQXs6Z6E+TlgX1U9kuSrwL4k9wHP\nA/cCVNXRJPuAY8AFYEdVXRxN+ZKkyxnkKZqngNtnGf8ecOdljtkF7Bq6OknSojmTVZIaZcBLUqMM\neElqlH8uWM3qTc+QXr28gpekRhnwktQoA16SGmXAS1KjDHhJapQBL0mNMuAlqVEGvCQ1yoCXpEYZ\n8JLUKANekhplwEtSowx4SWqUAS9JjRrkS7dfm+RQkieTHE3y4W78Q0lOJzncve7uO+aBJNNJjie5\na5QNSJJmN8jfgz8PvK2qXkxyNfCVJP/abftEVX20f+ckG4CtwG3ALwOPJvlVv3hbkq6sea/gq+fF\nbvXq7jXXNylsAfZW1fmqeg6YBjYNXakkaUEGugefZEWSw8A54EBVPd5tek+Sp5I8lOSGbmw1cLLv\n8FPdmCTpChoo4KvqYlVtBNYAm5K8AfgkcCuwETgDfGwhH5xke5KpJFMzMzMLLFuSNJ8FPUVTVT8A\nHgM2V9XZLvhfAj7FK7dhTgNr+w5b041deq7dVTVZVZMTExOLq16SdFmDPEUzkeT6bvl1wNuBZ5Ks\n6tvtncCRbnk/sDXJNUnWAeuBQ0tbtiRpPoM8RbMK2JNkBb0fCPuq6pEk/5BkI71/cD0B3A9QVUeT\n7AOOAReAHT5BI0lX3rwBX1VPAbfPMv7uOY7ZBewarjRJ0jCcySpJjTLgJalRBrwkNcqAl6RGGfCS\n1CgDXpIaZcBLUqMMeElqlAEvSY0y4CWpUQa8JDXKgJekRhnwktQoA16SGmXAS1KjDHhJapQBL0mN\nMuAlqVEGvCQ1auCAT7IiyTeSPNKt35jkQJJnu/cb+vZ9IMl0kuNJ7hpF4ZKkuS3kCv69wNN96zuB\ng1W1HjjYrZNkA7AVuA3YDDyYZMXSlCtJGtRAAZ9kDfB7wN/2DW8B9nTLe4B7+sb3VtX5qnoOmAY2\nLU25kqRBXTXgfn8N/BlwXd/Yyqo60y2/AKzsllcD/9W336lu7Cck2Q5s71ZfTPI94LsD1jNObsK+\nxk2rvdnXePmVJNuravdiTzBvwCf5feBcVT2R5I7Z9qmqSlIL+eCu6P8vPMlUVU0u5BzjwL7GT6u9\n2df4STJFX04u1CBX8G8G/iDJ3cBrgZ9P8o/A2SSrqupMklXAuW7/08DavuPXdGOSpCto3nvwVfVA\nVa2pqlvo/ePpf1TVHwL7gW3dbtuAh7vl/cDWJNckWQesBw4teeWSpDkNeg9+Nh8B9iW5D3geuBeg\nqo4m2QccAy4AO6rq4gDnW/SvIT/j7Gv8tNqbfY2foXpL1YJunUuSxoQzWSWpUcse8Ek2dzNep5Ps\nXO56FirJQ0nOJTnSNzb2s3yTrE3yWJJjSY4meW83Pta9JXltkkNJnuz6+nA3PtZ9vazVGedJTiT5\nZpLD3ZMlTfSW5Pok/5zkmSRPJ/ntJe2rqpbtBawAvg3cCrwGeBLYsJw1LaKHtwJvBI70jf0VsLNb\n3gn8Zbe8oevxGmBd1/uK5e7hMn2tAt7YLV8HfKurf6x7AwJc2y1fDTwOvGnc++rr70+AzwKPtPL/\nYlfvCeCmS8bGvjd6k0T/qFt+DXD9Uva13Ffwm4DpqvpOVf0Y2EtvJuzYqKovA9+/ZHjsZ/lW1Zmq\n+nq3/CN6f6ZiNWPeW/W82K1e3b2KMe8LXpUzzse6tyS/QO8C8dMAVfXjqvoBS9jXcgf8auBk3/qs\ns17H0FyzfMeu3yS3ALfTu9od+9662xiH6c3dOFBVTfTFKzPOX+oba6Ev6P0QfjTJE90seBj/3tYB\nM8DfdbfV/jbJ61nCvpY74JtXvd+txvZRpSTXAp8H3ldVP+zfNq69VdXFqtpIbxLepiRvuGT72PXV\nP+P8cvuMY1993tL9N/tdYEeSt/ZvHNPerqJ3e/eTVXU78D90f7TxZcP2tdwB3+qs17Pd7F7GeZZv\nkqvphftnquoL3XATvQF0vw4/Ru+vno57Xy/POD9B71bn2/pnnMPY9gVAVZ3u3s8BX6R3a2LcezsF\nnOp+gwT4Z3qBv2R9LXfAfw1Yn2RdktfQmym7f5lrWgpjP8s3SejdG3y6qj7et2mse0sykeT6bvl1\nwNuBZxjzvqrhGedJXp/kupeXgXcARxjz3qrqBeBkkl/rhu6kN0F06fr6GfhX5LvpPaHxbeADy13P\nIur/HHAG+F96P5HvA36R3t/IfxZ4FLixb/8PdL0eB353ueufo6+30PvV8CngcPe6e9x7A34d+EbX\n1xHgL7rxse7rkh7v4JWnaMa+L3pP2T3ZvY6+nBON9LYRmOr+f/wX4Ial7MuZrJLUqOW+RSNJGhED\nXpIaZcBLUqMMeElqlAEvSY0y4CWpUQa8JDXKgJekRv0fbbaO1uQAgAQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13f45952b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #predict array of action probabilities\n",
    "        probs = agent.predict_proba([s])[0] \n",
    "        \n",
    "        a = int(np.random.choice(n_actions, 1, p=probs))\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward = 23.27000\tthreshold = 26.0\n",
      "mean reward = 25.20000\tthreshold = 31.3\n",
      "mean reward = 27.07000\tthreshold = 31.0\n",
      "mean reward = 29.05000\tthreshold = 32.3\n",
      "mean reward = 29.45000\tthreshold = 32.6\n",
      "mean reward = 32.23000\tthreshold = 34.3\n",
      "mean reward = 36.17000\tthreshold = 42.0\n",
      "mean reward = 38.70000\tthreshold = 46.0\n",
      "mean reward = 39.87000\tthreshold = 48.3\n",
      "mean reward = 40.51000\tthreshold = 49.6\n",
      "mean reward = 40.93000\tthreshold = 46.0\n",
      "mean reward = 44.34000\tthreshold = 53.0\n",
      "mean reward = 44.42000\tthreshold = 54.0\n",
      "mean reward = 50.50000\tthreshold = 57.3\n",
      "mean reward = 50.40000\tthreshold = 60.3\n",
      "mean reward = 52.01000\tthreshold = 59.0\n",
      "mean reward = 55.80000\tthreshold = 61.0\n",
      "mean reward = 51.55000\tthreshold = 62.0\n",
      "mean reward = 61.30000\tthreshold = 69.3\n",
      "mean reward = 57.03000\tthreshold = 69.3\n",
      "mean reward = 71.20000\tthreshold = 84.6\n",
      "mean reward = 69.02000\tthreshold = 77.0\n",
      "mean reward = 76.66000\tthreshold = 89.0\n",
      "mean reward = 79.30000\tthreshold = 96.3\n",
      "mean reward = 91.96000\tthreshold = 103.8\n",
      "mean reward = 122.76000\tthreshold = 147.3\n",
      "mean reward = 120.94000\tthreshold = 142.6\n",
      "mean reward = 128.73000\tthreshold = 149.0\n",
      "mean reward = 174.22000\tthreshold = 196.1\n",
      "mean reward = 184.34000\tthreshold = 214.3\n",
      "mean reward = 171.76000\tthreshold = 191.3\n",
      "mean reward = 245.21000\tthreshold = 303.9\n",
      "mean reward = 270.19000\tthreshold = 313.3\n",
      "mean reward = 311.29000\tthreshold = 338.3\n",
      "mean reward = 366.37000\tthreshold = 421.9\n",
      "mean reward = 463.85000\tthreshold = 554.7\n",
      "mean reward = 427.23000\tthreshold = 518.2\n",
      "mean reward = 460.11000\tthreshold = 532.1\n",
      "mean reward = 489.00000\tthreshold = 587.6\n",
      "mean reward = 592.19000\tthreshold = 693.8\n",
      "mean reward = 730.51000\tthreshold = 947.1\n",
      "mean reward = 816.56000\tthreshold = 1000.0\n",
      "mean reward = 777.54000\tthreshold = 947.0\n",
      "mean reward = 757.91000\tthreshold = 983.3\n",
      "mean reward = 875.32000\tthreshold = 1000.0\n",
      "mean reward = 950.44000\tthreshold = 1000.0\n",
      "mean reward = 941.21000\tthreshold = 1000.0\n",
      "mean reward = 951.36000\tthreshold = 1000.0\n",
      "mean reward = 951.64000\tthreshold = 1000.0\n",
      "mean reward = 950.07000\tthreshold = 1000.0\n",
      "mean reward = 974.89000\tthreshold = 1000.0\n",
      "mean reward = 988.09000\tthreshold = 1000.0\n",
      "mean reward = 995.17000\tthreshold = 1000.0\n",
      "mean reward = 991.31000\tthreshold = 1000.0\n",
      "mean reward = 866.14000\tthreshold = 1000.0\n",
      "mean reward = 971.60000\tthreshold = 1000.0\n",
      "mean reward = 969.29000\tthreshold = 1000.0\n",
      "mean reward = 975.36000\tthreshold = 1000.0\n",
      "mean reward = 975.79000\tthreshold = 1000.0\n",
      "mean reward = 976.14000\tthreshold = 1000.0\n",
      "mean reward = 956.67000\tthreshold = 1000.0\n",
      "mean reward = 983.07000\tthreshold = 1000.0\n",
      "mean reward = 990.14000\tthreshold = 1000.0\n",
      "mean reward = 1000.00000\tthreshold = 1000.0\n",
      "mean reward = 1000.00000\tthreshold = 1000.0\n",
      "mean reward = 982.89000\tthreshold = 1000.0\n",
      "mean reward = 971.71000\tthreshold = 1000.0\n",
      "mean reward = 996.70000\tthreshold = 1000.0\n",
      "mean reward = 997.86000\tthreshold = 1000.0\n",
      "mean reward = 996.69000\tthreshold = 1000.0\n",
      "mean reward = 962.49000\tthreshold = 1000.0\n",
      "mean reward = 881.10000\tthreshold = 1000.0\n",
      "mean reward = 780.22000\tthreshold = 925.2\n",
      "mean reward = 950.85000\tthreshold = 1000.0\n",
      "mean reward = 994.72000\tthreshold = 1000.0\n",
      "mean reward = 982.70000\tthreshold = 1000.0\n",
      "mean reward = 951.89000\tthreshold = 1000.0\n",
      "mean reward = 994.52000\tthreshold = 1000.0\n",
      "mean reward = 973.92000\tthreshold = 1000.0\n",
      "mean reward = 997.78000\tthreshold = 1000.0\n",
      "mean reward = 991.73000\tthreshold = 1000.0\n",
      "mean reward = 988.23000\tthreshold = 1000.0\n",
      "mean reward = 988.46000\tthreshold = 1000.0\n",
      "mean reward = 993.33000\tthreshold = 1000.0\n",
      "mean reward = 989.46000\tthreshold = 1000.0\n",
      "mean reward = 986.25000\tthreshold = 1000.0\n",
      "mean reward = 980.88000\tthreshold = 1000.0\n",
      "mean reward = 994.02000\tthreshold = 1000.0\n",
      "mean reward = 954.08000\tthreshold = 1000.0\n",
      "mean reward = 966.02000\tthreshold = 1000.0\n",
      "mean reward = 940.06000\tthreshold = 1000.0\n",
      "mean reward = 972.89000\tthreshold = 1000.0\n",
      "mean reward = 957.33000\tthreshold = 1000.0\n",
      "mean reward = 945.30000\tthreshold = 1000.0\n",
      "mean reward = 973.52000\tthreshold = 1000.0\n",
      "mean reward = 963.02000\tthreshold = 1000.0\n",
      "mean reward = 952.67000\tthreshold = 1000.0\n",
      "mean reward = 982.83000\tthreshold = 1000.0\n",
      "mean reward = 968.64000\tthreshold = 1000.0\n",
      "mean reward = 987.30000\tthreshold = 1000.0\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "percentile = 70\n",
    "smoothing = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    #generate new sessions\n",
    "    sessions = [generate_session() for _ in range(n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "\n",
    "    threshold = np.percentile(batch_rewards, percentile)\n",
    "    \n",
    "    elite_states = batch_states[batch_rewards>=threshold]\n",
    "    elite_actions = batch_actions[batch_rewards>=threshold]\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #elite_states: a list of states from top games\n",
    "    #elite_actions: a list of actions from top games\n",
    "    \n",
    "    agent.fit(X=elite_states, y=elite_actions)\n",
    "\n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-27 10:49:24,611] Making new env: CartPole-v0\n",
      "[2017-08-27 10:49:24,611] Creating monitor directory ./monitor\n",
      "[2017-08-27 10:49:24,611] Starting new video recorder writing to C:\\Users\\Abdul\\Documents\\My Projects\\Reinforcement Learning\\week1\\monitor\\openaigym.video.2.8844.video000000.mp4\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-7b597b82edce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CartPole-v0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'./monitor'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msessions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;31m#upload to gym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-7b597b82edce>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CartPole-v0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'./monitor'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msessions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;31m#upload to gym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-cd448f4e44a4>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\wrappers\\monitoring.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\wrappers\\monitoring.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[1;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\wrappers\\monitoring.py\u001b[0m in \u001b[0;36m_reset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         )\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoder_version'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_path, frame_shape, frames_per_sec)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ffmpeg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`."
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "from gym import wrappers\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env = wrappers.Monitor(env,'./monitor')\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#upload to gym\n",
    "# gym.upload(\"./videos/\",api_key=\"\") #you'll need me later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part I\n",
    "\n",
    "### Tabular correntropy method\n",
    "\n",
    "You may have noticed that the taxi problem quickly converges from -10k to aroung -500 score (+- 500) and stays there. This is in part because taxi-v2 has some hard-coded randomness in the environment. Other reason is that the percentile was chosen poorly.\n",
    "\n",
    "### Tasks\n",
    "- __1.1__ (1 pt) Modify the tabular CEM (CrossEntropyMethod) code to plot distribution of rewards and threshold on each tick.\n",
    "- __1.2__ (2 pts) Find out how the algorithm performance changes if you change different percentile and different n_samples.\n",
    "\n",
    "```<YOUR ANSWER>```\n",
    "\n",
    "\n",
    "- __1.3__ (2 pts) Tune the algorithm to end up with positive average score.\n",
    "- __1.4 bonus__ (1 pt) Try to achieve a distribution where 25% or more samples score above +9.0\n",
    "- __1.5 bonus__ (2 pts) Solve and upload [Taxi-v1](https://gym.openai.com/envs/Taxi-v1) to the openai gym.\n",
    "\n",
    "It's okay to modify the existing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part II\n",
    "\n",
    "### Deep crossentropy method\n",
    "\n",
    "By this moment you should have got enough score on [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) to consider it solved (see the link). It's time to upload the result and get to something harder.\n",
    "\n",
    "* if you have any trouble with CartPole-v0 and feel stuck, feel free to ask us or your peers for help.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* __2.1__ Go to the [gym site](http://gym.openai.com/), register and obtain __api key__.\n",
    "* __2.2__ (1 pt) Upload your result to gym via gym.upload (see Results tab above, the line you need is commented)\n",
    "* __2.3__ (3 pts) Pick one of environments: MountainCar-v0 or LunarLander-v2 (or both) and solve it.\n",
    "  * For MountainCar, learn to finish it in __less than 180 steps__\n",
    "  * For LunarLander, learn to get reward of __at least +50__\n",
    "  * See the tips section below, it's kinda important.\n",
    "  \n",
    "  \n",
    "* __2.4__ (1+ pt) Devise a way to speed up training at least 2x against the default version\n",
    "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8)\n",
    "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
    "  * Experiment with amount of training iterations and learning rate of the neural network (see params)\n",
    "  \n",
    "  \n",
    "### Tips\n",
    "* Gym page: [mountaincar](https://gym.openai.com/envs/MountainCar-v0), [lunarlander](https://gym.openai.com/envs/LunarLander-v2)\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ whule R > threshold works alright.\n",
    "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
    "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
    "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)\n",
    "* 20-neuron network is probably not enough, feel free to experiment.\n",
    "* __Please upload the results to openai gym and send links to all submissions in the e-mail__\n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "* __2.5 bonus__ Try to find a network architecture and training params that solve __both__ environments above (_Points depend on implementation_)\n",
    "\n",
    "* __2.6 bonus__ Solve continuous action space task with `MLPRegressor` or similar.\n",
    "  * [MountainCarContinuous-v0](https://gym.openai.com/envs/MountainCarContinuous-v0), [LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2) (4+ points if it works)\n",
    "  \n",
    "* __2.7 bonus__ Use any deep learning framework of your choice to implement policy-gradient (see lectures) on any of those envs (4 +1 per env):\n",
    "  * CartPole-v0\n",
    "  * MountainCar-v0\n",
    "  * LunarLander-v2\n",
    "  * See __tips on policy gradient__ below.\n",
    "  \n",
    "\n",
    "* __2.8 bonus__ take your favorite deep learning framework and try to get above random in [Atari Breakout](https://gym.openai.com/envs/Breakout-v0) with crossentropy method over a convolutional network.\n",
    "  * Expect at least +10 points if you get this up and running, no deadlines apply ! \n",
    "  * __See tips below on where to start, they're cruicially important__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips on policy gradient\n",
    "\n",
    "* The loss function is very similar to crossentropy method. You can get away with using rewards as  __sample_weights__.\n",
    "* If your algorithm converges to a poor strategy, try regularizing with entropy or just somehow prevent agent from picking actions deterministically (e.g. when probs = 0,0,1,0,0)\n",
    "* We will use `lasagne` later in the course so you can try to [learn it](http://lasagne.readthedocs.io/en/latest/user/tutorial.html).\n",
    "* If you don't want to mess with theano just yet, try [keras](https://keras.io/getting-started/sequential-model-guide/) or [mxnet](http://mxnet.io/tutorials/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tips on atari breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's all the pre-processing and tuning done for you in the code below\n",
    "* Once you got it working, it's probably a good idea to pre-train with autoencoder or something\n",
    "* We use last 4 frames as observations to account for ball velocity\n",
    "* The code below requires ```pip install Image``` and ```pip install gym[atari]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-25 22:38:13,932] Making new env: BreakoutDeterministic-v0\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[1;32mimport\u001b[0m \u001b[0matari_py\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'atari_py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-2c3a58742454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbreakout\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_breakout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_breakout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Documents\\My Projects\\Reinforcement Learning\\week1\\breakout.py\u001b[0m in \u001b[0;36mmake_breakout\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_breakout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[1;34m\"\"\"creates breakout env with all preprocessing done for you\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mPreprocessAtari\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"BreakoutDeterministic-v0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPreprocessAtari\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Making new env: %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestep_limit\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'vnc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_limit\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTimeLimit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempting to make deprecated env {}. (HINT: is there a newer registered version of this env?)'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_entry_point\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mentry_point\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEntryPoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x={}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentry_point\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, require, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2303\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrequire\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2304\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2305\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2307\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2309\u001b[0m         \u001b[0mResolve\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentry\u001b[0m \u001b[0mpoint\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mits\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \"\"\"\n\u001b[0;32m-> 2311\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__name__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\envs\\atari\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matari\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matari_env\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAtariEnv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0matari_py\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)"
     ]
    }
   ],
   "source": [
    "from breakout import make_breakout\n",
    "\n",
    "env = make_breakout()\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the initial state\n",
    "s = env.reset()\n",
    "print (s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plot first observation. Only one frame\n",
    "plt.imshow(s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next frame\n",
    "new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after 10 frames\n",
    "for _ in range(10):\n",
    "    new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "< tons of your code here or elsewhere >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
