{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning with crossentropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DISPLAY=:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'bash' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-25 09:14:02,908] Making new env: Taxi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states,n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy = np.ones((n_states, n_actions))/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray,np.matrix)\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        a = int(np.random.choice(n_actions, 1, p = policy[s]))\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward = total_reward + r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s,a,r = generate_session()\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) is float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.68 s\n",
      "mean reward = -768.54000\tthreshold = -821.0\n",
      "Wall time: 2.5 s\n",
      "mean reward = -721.66000\tthreshold = -785.0\n",
      "Wall time: 2.5 s\n",
      "mean reward = -679.16000\tthreshold = -758.0\n",
      "Wall time: 2.48 s\n",
      "mean reward = -655.01600\tthreshold = -728.8\n",
      "Wall time: 2.79 s\n",
      "mean reward = -623.74000\tthreshold = -704.0\n",
      "Wall time: 2.52 s\n",
      "mean reward = -588.80400\tthreshold = -659.0\n",
      "Wall time: 2.52 s\n",
      "mean reward = -554.08400\tthreshold = -623.0\n",
      "Wall time: 2.44 s\n",
      "mean reward = -515.42000\tthreshold = -593.8\n",
      "Wall time: 2.36 s\n",
      "mean reward = -471.40800\tthreshold = -551.0\n",
      "Wall time: 2.81 s\n",
      "mean reward = -446.46800\tthreshold = -524.0\n",
      "Wall time: 2.25 s\n",
      "mean reward = -406.89600\tthreshold = -497.0\n",
      "Wall time: 2.45 s\n",
      "mean reward = -391.06000\tthreshold = -470.0\n",
      "Wall time: 2.3 s\n",
      "mean reward = -362.94400\tthreshold = -434.0\n",
      "Wall time: 2.27 s\n",
      "mean reward = -335.86000\tthreshold = -414.5\n",
      "Wall time: 2.29 s\n",
      "mean reward = -313.60000\tthreshold = -389.0\n",
      "Wall time: 2 s\n",
      "mean reward = -264.88000\tthreshold = -344.0\n",
      "Wall time: 2.03 s\n",
      "mean reward = -244.06000\tthreshold = -317.0\n",
      "Wall time: 2.26 s\n",
      "mean reward = -230.57200\tthreshold = -290.0\n",
      "Wall time: 2.02 s\n",
      "mean reward = -208.56400\tthreshold = -263.0\n",
      "Wall time: 1.96 s\n",
      "mean reward = -189.72000\tthreshold = -245.0\n",
      "Wall time: 2.25 s\n",
      "mean reward = -192.70800\tthreshold = -236.0\n",
      "Wall time: 1.99 s\n",
      "mean reward = -172.12000\tthreshold = -218.0\n",
      "Wall time: 2 s\n",
      "mean reward = -173.96400\tthreshold = -209.0\n",
      "Wall time: 1.78 s\n",
      "mean reward = -149.34800\tthreshold = -200.0\n",
      "Wall time: 1.74 s\n",
      "mean reward = -459.03600\tthreshold = -794.0\n",
      "Wall time: 1.7 s\n",
      "mean reward = -413.90400\tthreshold = -722.0\n",
      "Wall time: 1.63 s\n",
      "mean reward = -355.37200\tthreshold = -665.8\n",
      "Wall time: 1.46 s\n",
      "mean reward = -305.42000\tthreshold = -557.2\n",
      "Wall time: 1.11 s\n",
      "mean reward = -204.92400\tthreshold = -346.0\n",
      "Wall time: 875 ms\n",
      "mean reward = -143.79600\tthreshold = -206.2\n",
      "Wall time: 734 ms\n",
      "mean reward = -115.86400\tthreshold = -149.2\n",
      "Wall time: 563 ms\n",
      "mean reward = -71.65200\tthreshold = -102.2\n",
      "Wall time: 515 ms\n",
      "mean reward = -57.49200\tthreshold = -79.0\n",
      "Wall time: 474 ms\n",
      "mean reward = -41.16800\tthreshold = -59.0\n",
      "Wall time: 406 ms\n",
      "mean reward = -36.14400\tthreshold = -48.0\n",
      "Wall time: 375 ms\n",
      "mean reward = -27.89600\tthreshold = -34.0\n",
      "Wall time: 375 ms\n",
      "mean reward = -25.48400\tthreshold = -31.0\n",
      "Wall time: 344 ms\n",
      "mean reward = -19.34800\tthreshold = -26.8\n",
      "Wall time: 297 ms\n",
      "mean reward = -11.84000\tthreshold = -19.0\n",
      "Wall time: 328 ms\n",
      "mean reward = -12.68800\tthreshold = -14.0\n",
      "Wall time: 313 ms\n",
      "mean reward = -11.87600\tthreshold = -10.0\n",
      "Wall time: 297 ms\n",
      "mean reward = -9.99200\tthreshold = -11.0\n",
      "Wall time: 328 ms\n",
      "mean reward = -11.86400\tthreshold = -13.8\n",
      "Wall time: 313 ms\n",
      "mean reward = -9.51600\tthreshold = -8.0\n",
      "Wall time: 297 ms\n",
      "mean reward = -11.29600\tthreshold = -6.0\n",
      "Wall time: 313 ms\n",
      "mean reward = -15.92400\tthreshold = -15.8\n",
      "Wall time: 281 ms\n",
      "mean reward = -9.83200\tthreshold = -5.0\n",
      "Wall time: 282 ms\n",
      "mean reward = -9.84400\tthreshold = -6.8\n",
      "Wall time: 283 ms\n",
      "mean reward = -10.47200\tthreshold = -5.0\n",
      "Wall time: 281 ms\n",
      "mean reward = -10.76400\tthreshold = -9.0\n",
      "Wall time: 266 ms\n",
      "mean reward = -6.34800\tthreshold = -5.0\n",
      "Wall time: 281 ms\n",
      "mean reward = -12.73200\tthreshold = -4.8\n",
      "Wall time: 266 ms\n",
      "mean reward = -9.47600\tthreshold = -5.0\n",
      "Wall time: 281 ms\n",
      "mean reward = -10.94400\tthreshold = -5.0\n",
      "Wall time: 281 ms\n",
      "mean reward = -11.40400\tthreshold = -8.0\n",
      "Wall time: 250 ms\n",
      "mean reward = -5.47600\tthreshold = -2.0\n",
      "Wall time: 266 ms\n",
      "mean reward = -9.59600\tthreshold = -6.0\n",
      "Wall time: 297 ms\n",
      "mean reward = -14.25200\tthreshold = -13.0\n",
      "Wall time: 266 ms\n",
      "mean reward = -7.68800\tthreshold = -6.8\n",
      "Wall time: 250 ms\n",
      "mean reward = -3.75200\tthreshold = -3.0\n",
      "Wall time: 266 ms\n",
      "mean reward = -9.08400\tthreshold = -2.0\n",
      "Wall time: 266 ms\n",
      "mean reward = -7.68000\tthreshold = -1.0\n",
      "Wall time: 266 ms\n",
      "mean reward = -8.26000\tthreshold = -6.8\n",
      "Wall time: 250 ms\n",
      "mean reward = -3.32000\tthreshold = -1.0\n",
      "Wall time: 266 ms\n",
      "mean reward = -9.66000\tthreshold = -9.0\n",
      "Wall time: 254 ms\n",
      "mean reward = -8.95600\tthreshold = -8.0\n",
      "Wall time: 266 ms\n",
      "mean reward = -9.89600\tthreshold = -7.0\n",
      "Wall time: 453 ms\n",
      "mean reward = -4.94800\tthreshold = -4.0\n",
      "Wall time: 281 ms\n",
      "mean reward = -12.54000\tthreshold = -8.8\n",
      "Wall time: 344 ms\n",
      "mean reward = -7.70400\tthreshold = -4.8\n",
      "Wall time: 266 ms\n",
      "mean reward = -6.81600\tthreshold = -1.0\n",
      "Wall time: 266 ms\n",
      "mean reward = -10.77600\tthreshold = -10.0\n",
      "Wall time: 281 ms\n",
      "mean reward = -11.02800\tthreshold = -6.0\n",
      "Wall time: 250 ms\n",
      "mean reward = -7.19200\tthreshold = -4.8\n",
      "Wall time: 250 ms\n",
      "mean reward = -5.90800\tthreshold = -1.0\n",
      "Wall time: 281 ms\n",
      "mean reward = -11.24800\tthreshold = -6.8\n",
      "Wall time: 266 ms\n",
      "mean reward = -7.22800\tthreshold = -4.8\n",
      "Wall time: 297 ms\n",
      "mean reward = -11.99200\tthreshold = -3.0\n",
      "Wall time: 297 ms\n",
      "mean reward = -16.13600\tthreshold = -6.8\n",
      "Wall time: 250 ms\n",
      "mean reward = -9.30400\tthreshold = -5.8\n",
      "Wall time: 250 ms\n",
      "mean reward = -5.84000\tthreshold = -2.8\n",
      "Wall time: 265 ms\n",
      "mean reward = -9.45200\tthreshold = -5.0\n",
      "Wall time: 270 ms\n",
      "mean reward = -9.22000\tthreshold = -5.8\n",
      "Wall time: 250 ms\n",
      "mean reward = -5.60800\tthreshold = -0.8\n",
      "Wall time: 250 ms\n",
      "mean reward = -3.68000\tthreshold = 2.0\n",
      "Wall time: 313 ms\n",
      "mean reward = -23.06800\tthreshold = -8.8\n",
      "Wall time: 234 ms\n",
      "mean reward = -5.82000\tthreshold = -3.0\n",
      "Wall time: 250 ms\n",
      "mean reward = -4.32400\tthreshold = -4.0\n",
      "Wall time: 266 ms\n",
      "mean reward = -8.91600\tthreshold = -6.8\n",
      "Wall time: 266 ms\n",
      "mean reward = -8.86800\tthreshold = -7.0\n",
      "Wall time: 266 ms\n",
      "mean reward = -7.58400\tthreshold = -6.0\n",
      "Wall time: 266 ms\n",
      "mean reward = -9.78800\tthreshold = -8.8\n",
      "Wall time: 266 ms\n",
      "mean reward = -11.91600\tthreshold = -5.8\n",
      "Wall time: 266 ms\n",
      "mean reward = -11.72400\tthreshold = -6.8\n",
      "Wall time: 266 ms\n",
      "mean reward = -6.82800\tthreshold = -4.8\n",
      "Wall time: 234 ms\n",
      "mean reward = 0.91200\tthreshold = 2.0\n",
      "Wall time: 250 ms\n",
      "mean reward = -9.98800\tthreshold = -4.8\n",
      "Wall time: 250 ms\n",
      "mean reward = -5.08800\tthreshold = -4.8\n",
      "Wall time: 250 ms\n",
      "mean reward = -7.21200\tthreshold = -1.8\n",
      "Wall time: 234 ms\n",
      "mean reward = -4.82000\tthreshold = -0.8\n"
     ]
    }
   ],
   "source": [
    "n_samples = 250  #sample this many samples\n",
    "percentile = 25  #take this percent of session with highest rewards\n",
    "smoothing = 0.01  #add this thing to all counts for stability\n",
    "\n",
    "reward_list = []\n",
    "threshold_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    %time sessions = [generate_session() for i in range(n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "    \n",
    "    threshold = np.percentile(batch_rewards, percentile)\n",
    "    \n",
    "    index_threshold = [i for i, j in enumerate(batch_rewards) if j > threshold]\n",
    "    \n",
    "    elite_states = batch_states[index_threshold] \n",
    "    elite_actions = batch_actions[index_threshold]\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #hint on task above: use np.percentile and numpy-style indexing\n",
    "    \n",
    "    #count actions from elite states\n",
    "    elite_counts = np.zeros_like(policy)+smoothing\n",
    "    \n",
    "    for i,j in zip(elite_states, elite_actions):\n",
    "        elite_counts[i, j] = elite_counts[i, j] + 1\n",
    "    \n",
    "    for i in range(elite_counts.shape[0]):\n",
    "        elite_counts[i,] = elite_counts[i,]/np.sum(elite_counts[i,])\n",
    "            \n",
    "    policy = elite_counts\n",
    "    \n",
    "    reward_list.append(np.mean(batch_rewards))\n",
    "    threshold_list.append(threshold)\n",
    "    \n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd8VGX2+PHPM5OZdEINLfSm9BIiTVYECaAURb8iIOiK\nuJZVV9SVdV3L2ii7iO6KBdkV5ScuTUEIAgFESoCAYKRKTSEhtPQ+8/z+yCQkZob0TDJz3q9XXsw8\n987ccyHMmfOcW5TWGiGEEO7N4OwAhBBCOJ8kAyGEEJIMhBBCSDIQQgiBJAMhhBBIMhBCCIEkAyGE\nEEgyEEIIgSQDIYQQgIezAyirxo0b67Zt2zo7DCGEqFMOHDhwWWvdpLT16kwyaNu2LZGRkc4OQwgh\n6hSl1PmyrCfTREIIISQZCCGEkGQghBACSQZCCCGQZCCEEAInJgOl1Cil1Aml1Cml1EvOikMIIYST\nkoFSygj8GxgNdAUeUEp1dUYsQgghnHeeQQhwSmt9BkAptRwYDxx1UjxCCBdksVo4fvk4e+P24m/2\n596u96KUckosVm3lvxH/xWQ0MaX/FAyqds3SOysZtARiijyPBW5xUixCOE12XjbLf1lOak4qYzuP\npU39NqW+JiYphh9O/0C6TudyxuX8n8z8P7PzspncYzJTe07Fy8OLHEsOy39ZzheRXzC171Sm9Z5W\npg/DPGseBmWw+4F1LfMa35/+ng2/bsDT6Mnbw9+mia/jE1xzLDnsjN7J+pPrOXr5KPd3u58Huj+A\np4dnsfWy8rI4e+0ssSmx9Gjag2Z+zey+35WMK0TERmA2munRtAdNfZsW2yeL1cLmM5tZfHAxm05v\nIjUntXDZw70f5qO7PsJsNAOQlJXE3ti9BHgF0NinMU19m+Lv6V/q3095aK355ug3PLHyCRJIAODN\nnW+ycNxCQjuEFsaenZdN5IVIdkbv5MilI4X/tlcyr3DiqRN4GKr341pprat1A3Y3qtS9wCit9Qzb\n8weBW7TWT/1mvZnATIDWrVv3O3++TCfSCeFQnjWPY5eOcfzycY5fPk49z3o80f8JTEZT4TpfRX3F\nv/b/i2Z+zejQoANNfZsSkxLD6WunSUxPZGjroUy4aQIDggZw+OJhPtv1GeuPr8fgYwDj9Q9Sk9GE\n2Wgu9oHarn47BrcazKBWg9gbt5d5O+eRmJlYuLx309482OtBHg9+HG+Td+H4kYQjvP7t64RHh3PV\nfLXYPvmZ/Wji04T65vokZyZzJvUMjb0bE9o6lHUn1pFCCmQC3nCz582sfHglXZt2JdeSy9XMq4Uf\nOonpieyL2ce6w+v4NeNXvLU3M/rP4PGBj2M2mvn2xLesOrKKPXF7sGgLfgY/snQWDb0b8t8J/2V0\np9FcSr/Edye/Y1fMLi6kXiAmKYZTV0+RZc3CqI34Wn1JMabQzK8ZD/V6iLScNI4mHuXIxSMkZiWi\nuf551K95P0Z3HI2PyYcrmVdITE9kf9x+jl85Xmz/G/s0pk1AGwK8AgjwDGD/hf3EpsTirb2pf6E+\nGScySD6ajO8AX9KD0xnaZij/HPlPlh5eyqcHPyUzL7PY+9XLqUf/Fv0Z3Wc09b3qYzKaMCgDVzOu\ncv7SeS4mX6R3q94M6zSMnk17Ep0czc7onUTERqDRNPZpTCPvRlzOuMyxy8fYe3YvF7IvwBXon9af\n87HnSeyWCPWhmV8zrLlW0tLTyDRkog35+9/Q2BBTjonc5FyyrmZx4eMLBPgEVOh3Xil1QGsdXOp6\nTkoGA4HXtNahtuezAbTW7zh6TXBwsJbLUQiA+Ph4MjIyaNCgAQEBAeTk5hB+Ipy1J9aScDWB3JRc\nMi9nUl/XZ1CbQfTp0ofUnFQ+O/QZ2zO3k2HKKPZ+Q1oNYfm9y2nu35y/hv+Vd3a9g392/rfDdFM6\nVoMVD6sH9a318TZ4c8FwAQsWjNqIRVnASn6dmwptgtrQq2cvLFYLFxIukHApAWVQeHt74+3jzWUu\nk5CXULhtdVahdinMmWay2mRh7G7E0sJCoGcgc0Ln0DOwJ48ve5x96fsAMMYZ6WbuRk//noT9L4wr\nMVdoVL8RaWlpZGdn579pO2Ag0Bk4DT3SevDgoAf5YM8HxHSOARN4WD3IM+WV/Mu1APHgn+RPmmca\nuoMu1llUiQp9XMNJIA5oAupehQ7UtDC0IN4aj0bjiy8kQ/rFdEjKj8Nw3oCH9iAnKAf/UH9SA1Px\nsHhgTbRiTbRiSjVxS+dbmDB8AlmNs9hwagMRsRFYtRUvoxdkQHZMNvqcxnjBiMVqgabg1dYLr8Ze\n5BnzyPPII/dyLpZIC6YzJoL7BHPTTTfRunVr3n77bfo91I+fWv9EtiUbD4MH3qe8sf5kpWOXjtRr\nVo80YxpRV6PIa5oHfnZ++TSQC5iLPLcVJX4efpiMJpKzk7FixaANGJIN5CXkEZgUyJKnl3Dn6Du5\nfPkyEyZOYFfGLnw7+5Kelo6HyYMmvk1IPpJMxrEMyAAvLy969OhB7969mTdvHgEBrpkMPMj/dRpO\n/q/UfmCy1vqIo9dIMhBXr17lb3/7G4sWLcJqtUILoA9wM/n/cS1ANuDz2xcCvoAneF7wpHFsY64c\nvUJWXBZ0ATVe4eXhRTtzO47mHoWD0Pxgc+r710crTa7KJTMpk7TUNNLS0rCarNARaAPtvdvz+8G/\n555R97Bs2TLef/99UlOvT0t06tQJDw8PoqOjSU9Pzx/0BYLAK8eLx8Y9xp/+9CcCAwPZvHkzq1at\nYsX+FWQOycyfTAXIgWYxzZgzYQ6T7pqE2Zz/SZSdnc2qVavYvHkzgYGBBAUFERgYSHZ2Nunp6WTl\nZjF2zFg6duwI5E9XfPHNF/xt49/Is+bhZ/DDz8MPQ6aBrKtZZFzOoF/bfjz2+8e47bbbOHHiBDOf\nm8nOaztBgX+8Pw/e+SChoaE0adKEhg0bEhsby6pvV/Fl3JekNkiF08BxIAFCQkK46667GDlyJO3a\ntaNx48ZkZGTw5Zdf8u9//5tfTv5C88bNuXvC3QwdOpQtW7bw9ddfk5qaSrNmzbj77rsZcvsQVvxv\nBd+s+IbGjRszffp0Ro4cyZAhQ0hNTWXTpk1s3LiRxMREPDw8MJlMtG7dmlGjRnHbbbfh53f9E/3v\nf/87f/vb35jz5Ryyg7I58NkB1i1bx/bt27n11lsL18vOzmb9+vV8u+VbsvKy0EpjMBpo0agF7Zu3\np0mjJhw6d4hd53dxMuUkV09fJfdMLlzienLwApWrGD1yNL///e8ZO3Zs4b9bwTZeeOEFjh07xpQp\nU7jvvvvw9fVFa01CQgLJycl07NgRD4/KTw3V6mQAoJQaA7wHGIElWuu3brS+JAP3lZOTw2effcYr\nr7zC1eSr3Pr0rZxtdJaY3Bg8tAc3G24m2C+YgY0H0rlNZ9p3bI85wMyxS8fYdGwTP575ET8PP168\n7UWG3TQMoPA/3Q8//MCiFYvY0WwHNIFWx1qxcNJCxo8fj8Fgv8FnsVjIzc3FarXi41M881y5coUv\nvviCJk2aMGzYMFq0aFG4vaSkJJKTk8nMzCQzM5N27drRoEGDEu+fkZHBN99+w/z187mmr/HmvW8y\necJkpzQ+tdZs3LiR9PR07rrrLry8vByul5SURG5uLrm5uXh7e9OwYcMbvm9cXBwtWrQo9veckZHB\n2rVrWbVqFRs2bCAjIwM/Pz9mzZrFrFmz8Pev+Hx+bm4uISEhxMfHM2vWLF588UXefPNNXn755Qq/\nJ+T/Ppw/f55jx46RlZWFh4cHHh4e9O7dm5YtW5b+BtWs1ieD8pJk4H4yMzNZvHgx8+bNIyYmhl5j\ne5E+LJ1TKafo17wfj/Z9lEndJxHgVbHyuajzF87z06mfGH/reKcdbSKKy8jIYM+ePfTo0YPAwMAq\nec9Dhw7Rv39/8vLyGDFiBBs3bsRoNFbJe9dWkgxEnZOXl8fatWvZv38/hw4dYu/evVxLuka3O7vR\nJLQJ269sp01AGxaOWsi4LuPkQ1tUyNy5c1myZAnbt2+nWTP7Ryy5EkkGok45dOgQjzzyCAcPHsTY\nwEjLgS3xutmLSw0vcS33Gj4mH5695VleHvoyPqbfNgWEKB+ttdt8mShrMqgzN7cRrunatWvMmzeP\nuXPnEnBTAM3eakZCbgLRRONn9mNMpzHc1/U+xnQaI0lAVBl3SQTlIclA1DitNTt27ODTTz9l5cqV\nZGdnM3HmRH7o8ANms5mFAxcypPUQejbtWe0n2ggh8sn/NFHj/v73v/Pqq68SEBDAjBkzGDdlHI/t\newxyYPODm+ncqLOzQxTC7UgyEDVqw4YNvPbaa0yZMoVPPvmETDIZ9vkwLmdcZtv0bZIIhHCS2nWl\nJOHSTp8+zZQpU+jVqxeffvopcZlxDPxsICevnGTN/WsIblFqj0sIUU2kMhA1IiMjg4kTJ6KUYvXq\n1UQmRjLh6wkYlIGt07cyqNUgZ4cohFuTZCCqncVi4cEHH+Tnn39m/fr1nOMco74YRbv67Vg/eT0d\nGnZwdohCuD1JBqJaaa158sknWb16Ne+99x5Dhw+lx6IetK3flt2P7Kaht+NLFgghao4kA1Gt3njj\nDT7++GNeeuklnnnmGZ7f9Dxnk87yw0M/SCIQohaRBrKoNitWrOC1117joYce4u233ybyQiQLIhbw\nWL/HGNpmqLPDE0IUIZejENUmNDSUs2fPcuTIETBA/0/7k5ieyLEnj1XJxeWEEKUr6+UopDIQ1SI9\nPZ3t27czbtw4TCYT7+99n8MXD/PvMf+WRCBELSTJQFSLrVu3kpOTw5gxY7iaeZU3f3yTUR1HcffN\ndzs7NCGEHdJAFtViw4YN+Pn5MWTIEF7a9hIp2SnMHTHX2WEJIRyQykBUOa01GzZs4I477iA2PZZ/\n7fsXD/V6iB5Nezg7NCGEA5IMRJU7cuQI0dHRjBkzhr+E/wUPgwdvDHvD2WEJIW5AkoGochs2bACg\nWb9mfH3ka2YNnEXLes6/F6wQwjFJBqLKbdiwgV69erEmeg0BngG8OPhFZ4ckhCiFJANRpZKTk9m5\ncyejx4xm05lNjGg/An9Pf2eHJYQohSQDUaU2b96MxWKh2++6EZsSy8gOI50dkhCiDCQZiCq1YsUK\nGjRoQKJ/IgB3tL/DyREJIcpCkoGoMmfPnmXlypU88sgjbD23lU4NO9GuQTtnhyWEKANJBqLKLFiw\nAKPRyB+e+gPbzm2TqkCIOkSSgagSV65c4bPPPmPy5MnE6BgycjOkXyBEHSLJQFSJDz/8kIyMDJ5/\n/nk2n96MURkZ1m6Ys8MSQpSRJANRaZmZmXzwwQeMGTOG7t27s+nMJgYEDaCeZz1nhyaEKCNJBqLS\nli5dyqVLl3jhhRe4nHGZAxcOyBSREHWMJAM3lpKSQlXc3Ojrr7+me/fu/O53vyP8TDgaLclAiDpG\nkoGbioqKIjAwkO+++65S72OxWIiMjGTo0KEopQg/G06AZwDBLUq9sZIQohaRZOCmXnnlFbKzs4mO\njq7U+5w4cYLU1FRCQkIA2BWzi0GtBuFhkFtlCFGXSDJwQ3v37uXbb78FICMjo1LvtX//fgD69+9P\nUlYSRy8dZVCrQZWOUQhRsyQZuKG//vWvNG7cGMi/V3Fl7Nu3D39/f7p06UJEbAQAA4MGVjpGIUTN\nkmTgZrZt28aWLVuYPXs2Xl5eVVIZBAcHYzQa2ROzB4MyENIypIqiFULUFEkGLi4+Pp7p06fz/PPP\n89FHH/HnP/+Zli1b8vjjj+Pr61upyiA7O5tDhw7Rv39/APbE7qFHYA+5ZLUQdZB0+VzcU089xdq1\na/Hw8CArKwuAjz/+GG9v70ong8OHD5Obm0tISAgWq4WI2Aim9pxaVaELIWqQVAYubN26daxevZo3\n3niD9PR0oqOj2bdvH48++igAPj4+lZomKto8PnrpKKk5qdIvEKKOksrARaWnp/PUU0/RtWtXZs2a\nhcFgoFWrVrRq1apwncpWBvv27aNp06a0atWKsANhAHIkkRB1VKUqA6XUPKXUcaXUz0qpNUqp+kWW\nzVZKnVJKnVBKhRYZ76eUirIte18ppSoTg8h3+fJlPv74Y/bv309ubi6vvfYa0dHRfPzxx5jNZruv\nqYrKICQkBKUUe2L3EOgbSPsG7Sv8fkII56lsZbAZmK21zlNKzQFmA39WSnUFJgHdgBbAFqVUZ621\nBVgEPArsBTYAo4CwSsbh1rTWTJ06le+//x7I/5DPyspixowZDBkyxOHrfH19uXLlSoW2mZKSwvHj\nx3nggQcA2B2zm4FBA5HcLkTdVKlkoLXeVORpBHCv7fF4YLnWOhs4q5Q6BYQopc4B9bTWEQBKqaXA\nBCQZVMqnn37K999/z9tvv03Hjh3ZuXMnsbGxzJkz54av8/HxISYmpkLbPHDgAFprQkJCuJxxmV+v\n/sqMvjMq9F5CCOeryp7B74GvbY9bkp8cCsTaxnJtj387LirozJkzPPfccwwfPpw///nPGAwG7rvv\nvhu+RmvNlz9/yaabN+EfXbHDQPft2wdAcHAwu2N2A3KymRB1WanJQCm1BWhmZ9HLWutvbeu8DOQB\ny6oyOKXUTGAmQOvWravyrV2C1Wrl4YcfxmAwsGTJEgyG0ltAVzOv8vj6x/nfkf+BCfCt2Lb3799P\nhw4daNSoEXsO7cHD4CEXpxOiDis1GWitR9xouVLqIeAuYLi+fj3kOKBVkdWCbGNxtse/HXe07U+A\nTwCCg4Mrf61lF/Phhx+yY8cOlixZUqZkGX4mnOnfTOdi+kWeH/g88/fMJycvp0LbPn78ON27dwcg\nKjGKrk264m3yrtB7CSGcr7JHE40CXgTGaa2LHpayFpiklPJUSrUDOgH7tNbxQIpSaoDtKKJpwLeV\nicFdXbp0iVdeeYURI0bw0EMP3XDdrLwsZn0/ixFfjMDf05+9M/by8tCXAcix5JT7ngZaa86ePUu7\ndu0ASEhLoIV/iwrthxCidqhsz+BfgCew2XYUSYTW+g9a6yNKqf8BR8mfPnrSdiQRwBPAfwFv8hvH\n0jyugFdeeYXU1FQWLlx4wyN4LqReYNSXo4hKjOLJ/k8y9465+Jh8SM+xnV9ggKysLLy9y/6t/vLl\ny2RkZBRLBt0Du1dqf4QQzlXZo4k63mDZW8BbdsYjAfnkqIRDhw7xySef8PTTT9O1a1eH61m1lWlr\npnH62mnWT17PmE5jCpeZjbZzD4z5J6iVJxmcPXsWgLZt26K15mLaRZr6Nq3YzgghagU5A7mO0Vrz\nzDPP0LBhQ1599dUbrrtgzwLCz4bz6dhPiyUC4PrNZ4zlv6fBuXPnAGjXrh3Xsq6Ra82lmZ+9YwyE\nEHWFJIM6ZtmyZezYsYOPPvqIBg0aOFzvUMIhZofP5u6b7uaRPo+UWK6UwogRi9FS7ktSFK0MYtPy\njxSWykCIuk0uVFeHhIeHM2PGDAYOHMiMGY5P8ErPSWfyqsk08W3Cp2M/ddhT8DB4FE4TlcfZs2dp\n1KgR/v7+JKQlAEhlIEQdJ8mgjti9ezfjxo2jU6dOfPfddxiNRrvrHYw/SPCnwRy/fJzPJ3xOI59G\nDt/TpExgqNg0UUHz+GL6RQCa+kllIERdJsmgDvjpp58YM2YMLVu2ZPPmzTRs2LDEOharhXd3vsst\ni28hNTuVzQ9uZkT7G54igsloqnBl0LZtWwAuptmSgUwTCVGnSTKo5bKzs5k0aRL+/v5s2bKFZs3s\nT8fMDp9d2CP4+fGfGd5+eKnvbTKYyt1AtlqtxSqDhLQETAYTDbwd9y+EELWfNJBrublz53Ly5EnC\nwsIcnmW85cwW5u2ex2P9HmPRnYvKfOVQs9Fc7sogISGBnJyc65VB+kUCfQMxKPleIURdJv+Da7HT\np0/z1ltvcd999zFq1Ci761zJuML0b6ZzU+Ob+GfoP8t1CWlPD89y9wwKjiQqWhlI81iIuk8qg1pK\na81TTz2F2WxmwYIFDtd5dN2jXEq/xHcPfIePyadc2zB7lL8y+G0yuJh+UZKBEC5AKoNaauXKlWzc\nuJE333yTli3tX+V7WdQy1hxfw9vD36ZP8z7l3oanh2e5k0HBCWdt2rQBbJWBryQDIeo6SQa11Pz5\n8+nWrRtPPPGE3eUWq4XXf3idvs378tzA5yq0DbOHGeWhyj1N1KxZM7y9vbFqK4npiXJYqRAuQJJB\nLZSUlERkZCQTJ07Ew8P+TN7Koys5dfUUfxnylwo3b81GM0azsdyVQUHz+GrmVfKseXJYqRAuQJJB\nLbR9+3asVivDh9s/PFRrzTs736FLoy7cffPdFd6OyWDCYDKUuzIo7BfYzjGQnoEQdZ8kg1ooPDwc\nHx8fBgwYYHd52KkwDl88zEtDXqrUIZ1moxmDh6HMlUFeXh7R0dFy9rEQLkiSQS0UHh7O0KFDMZvN\ndpe/s/MdWtVrxeQekyu1HZPRVK6eQVxcHBaLpXCaSK5LJITrkGRQy8TFxXHs2DGHU0Q/nv+RndE7\neWHQC9fvSVBBZqMZZVRlrgxKHFYql6IQwmVIMqhltm7dCuAwGbyz8x0a+zTmkb4lL0tdXiaDCTzK\nfmhp0UtXQ35lYDaaqe9Vv9KxCCGcS5JBLRMeHk6jRo3o1atXiWWHEg4RdiqMZ295ttwnmNljNprL\ndQbyuXPnUEoVXhbjYnr+Hc7Kc9azEKJ2kmRQi2it2bJlC7fffjsGQ8l/mnd3vou/2Z8nQ56sku2Z\nDCa0QZerMggKCirsZSSkJUjzWAgXIcmgFjl58iRxcXF2p4h+vfIrK46u4In+T1TZtExFKoOCKSKQ\nS1EI4UokGdQi4eHhgP1+wdxdczEZTDw74Nkq257JaMKqrGWqDPLy8jh06BBdu3YtHLuYdlGax0K4\nCEkGtcimTZto3bo1HTp0KDYelxLH54c/55E+j1TpN3Gz0YxVWcnIyEBrfcN1Dxw4QGpqKrfffjtA\n4aUopDIQwjVIMqglDh8+zLp167jvvvtKNGT/secfWLWVFwa/UKXbNBnyKwOLxUJOTs4N1y04yum2\n224D8i+dbdEWqQyEcBGSDGoBrTXPPvssDRo04OWXXy627ErGFT458AkP9HiAtvXbVul2zUYzFixA\n6YeXbtu2je7duxMYGAjICWdCuBpJBrXAmjVr2L59O2+88QYNGhS/feQH+z4gPTedlwa/VOXbNRlN\n+Q9KaSJnZ2ezc+fOwikikEtRCOFqJBk4WVZWFs8//zzdu3dn5syZxZalZqfy/t73Gd9lPN0Cu1X5\ntgvPYC7lngb79u0jMzOTYcOGFY4VVAYyTSSEa5A7nTnZggULOHv2LJs3by5xuepPDnzCtaxrzB4y\nu1q2bTLYKgPjjSuDrVu3opTid7/7XeGYXLFUCNcilYETJScn8+677zJ27FhGjBhRbFl2Xjb/2PMP\nbm93O7cE3VIt2y+sDAw3rgy2bt1K3759i01hJaQl4Gn0pJ5nvWqJTQhRsyQZONGHH35ISkoKr7/+\neollnx/+nPi0+GqrCqBIz+AGlUFGRgYRERHFpojg+glncikKIVyDJAMnyczM5L333iM0NJQ+fUre\nv3jxwcX0adaH4e3sX7CuKpSlZ7B7925ycnKKNY/Bdl0iaR4L4TIkGTjJkiVLSExMZPbskt/8E9MT\n2X9hP/fcfE+1fvMu2jNwlAy2bduG0WhkyJAhxcavZl6lkXejaotNCFGzJBk4QW5uLvPmzWPgwIEM\nHTq0xPJNpzcBMLrj6GqNo2jPwNE00datWwkJCcHf37/YeFJWEgFeAdUanxCi5kgycILly5dz/vx5\nZs+ebfebf9ipMAJ9A+nTvOT0UVUqbZooMzOTyMhIuwkrOSuZ+p5yHwMhXIUkgxpmtVqZM2cO3bt3\n58477yyx3GK18P2p7wntEFqp+xuXRWkN5IMHD5KXl8fAgQOLjWutpTIQwsXIeQY1LCwsjCNHjrB0\n6VK79yyIvBDJlcwr1T5FBNcrA6PZaLcyiIiIAOCWW4of2pqVl0WuNVfucCaEC5HKoIbNnz+foKAg\nJk2aZHd52KkwDMrAyA4jqz2Wggayl4+X3cogIiKCtm3b0qxZ8RPLkrKSAAjwlMpACFchyaAGRUZG\nsn37dp599llMJpPddcJOhRHSMoRGPtV/pE5BZeDp4+mwMhgwYECJ8eTsZACpDIRwIZIMatD8+fOp\nV68ejz76qN3llzMusz9uf41MEcH1noG9ZBAXF0dsbKzdZFBYGUjPQAiXIcmghpw7d44VK1bw2GOP\nUa+e/Us4bDq9CY1mVMdRNRJTQWVg9jaXmCbau3cvgP3KIEsqAyFcTZUkA6XULKWUVko1LjI2Wyl1\nSil1QikVWmS8n1IqyrbsfeUm1zNYsGABBoOBZ555xuE6YafCaOzTmOAWwTUSU0HPwOxtLlEZRERE\nYDab6d27d4nXFVQGkgyEcB2VTgZKqVbASCC6yFhXYBLQDRgFfKiUMtoWLwIeBTrZfmrma7ATHT16\nlMWLFzN58mRatmxpd52EtARWHl3JhC4Tqv2Q0gKFlYFXycogIiKCvn374unpWeJ1BT0DaSAL4Tqq\n4lNnAfAiUPQmuuOB5VrrbK31WeAUEKKUag7U01pH6Pyb7i4FJlRBDLXWtWvXGD9+PP7+/rz11lsO\n15uzcw65llz+POTPNRZbQc/Aw8ujWGWQm5tLZGSk3SkikMpACFdUqfMMlFLjgTit9eHfzPa0BCKK\nPI+1jeXaHv923CVZLBYmT57M+fPn2bZtG0FBQXbXi0+N56MDH/Fgrwfp2LBjjcVXUBmYPE0kZyQX\njkdFRZGZmekwGSRnJWNURnxMPjUSpxCi+pWaDJRSWwB7dzB5GfgL+VNE1UIpNROYCdC6devq2ky1\n+ctf/sLGjRv5+OOPGTx4sMP15uzKrwr+eutfazC66z0DD8/ilUHByWY3qgzqe9WXy1cL4UJKTQZa\n6xH2xpVSPYB2QEFVEAQcVEqFAHFAqyKrB9nG4myPfzvuaNufAJ8ABAcHa0fr1Ua//vorc+fOZebM\nmSVuZ1lUfGo8Hx/4mGm9ptGhYYcajPB6ZWAvGTRv3txhAk7OTpbDSoVwMRXuGWito7TWgVrrtlrr\ntuRP+fTjl5OGAAAYaElEQVTVWicAa4FJSilPpVQ78hvF+7TW8UCKUmqA7SiiacC3ld+N2mfVqlUA\n/PWvN/62/+7Od/OrgqE1WxXA9Z6B0WQs1kAuONnM0Tf/gspACOE6quWwFa31EeB/wFFgI/Ck1tpi\nW/wEsJj8pvJpIKw6YnC21atX079/f1q1auVwnZ8v/syHkR/ycO+Had+gfQ1Gl69gmshoNpKbm0tu\nbi6xsbH8+uuvN5zWSs5OliOJhHAxVXahOlt1UPT5W0CJw2e01pFA96rabm0UHR3N/v37effddx2u\nk2fN4+FvH6ahd0PeHeF4vepkNBgxKAMGU/53goyMDDZu3AjAqFGOj/hNykqiU8NONRKjEKJmyFVL\nq8Hq1asBmDhxosN15u+ez8H4g6y8b2WNXIfIEbPRXJgM0tPTCQsLIygoiK5duzp8TXKW9AyEcDVy\nOYpqsGrVKnr27EnHjvYPEz126RivbX+Ne7vey8SujhNGTTAZTCiP/N5AcnIyW7ZsYfTo0Tc8Uigp\nK0lubCOEi5FkUMUSEhLYtWvXDauCmd/NxNfsy79G/6sGI7PPbDSjjPkf/Fu2bCElJeWGU0QWq4XU\nnFSpDIRwMTJNVMW++eYbtNbcc889dpf/fPFndkbv5L3Q92jq17SGoyvJZLxeGaxatQoPDw+GDx/u\ncP2U7BRAzj4WwtVIZVDFVq1aRefOnenWrZvd5ct+XoZRGZncY3INR2af2WgG21WjduzYwaBBgwgI\ncPytX65LJIRrkmRQhS5dusS2bduYOHGi3Tl3q7by1S9fEdoxlCa+TZwQYUkmgwltyD+fT2vN6NE3\nvpeCXJdICNckyaAKzZ8/H6vVyrRp0+wu//H8j8SkxDClx5Qajswxs9Fc7LegtGRQcC8D6RkI4Vok\nGVSRhIQEPvjgA6ZMmcJNN91kd51lUcvwNfkyvsv4Go7OMZPRhFVZAWjevDk9e/a84fpSGQjhmiQZ\nVJF3332XnJwcXn31VbvLs/OyWXF0BXfffDe+Zt8ajs4xs9GMVvnTRKNGjSr14nNy/2MhXJMcTVQF\nYmNj+eijj5g+fbrDcwvCToWRlJVUq6aI4HrP4Nlnn+Xhhx8udf3C+x9LA1kIlyLJoAq8/fbbWK1W\nXnnlFYfrLItaRqBvICPa270IrNOYjWZyrbksWLCgTOtLz0AI1yTTRJUUExPD4sWLmTFjBm3btrW7\nTmJ6IutOrOP+bvfjYahd+ddkNJFjySnz+klZSfiafGvdfgghKkeSQSWtWbOG3NxcnnvuOYfrLIxY\nSI4lhyf7P1mDkZVNQWVQVsnZydIvEMIFSTKopPXr19OlSxeHvYLkrGT+vf/f3HPzPXRp3KWGoyud\nyVD+ykCmiIRwPZIMKiE9PZ3t27dz5513OlxnUeQikrOTmT1kdg1GVnZmo5lcS9krA7mxjRCuSZJB\nJYSHh5OTk8OYMWPsLs/MzWRBxAJGdhhJvxb9aji6silvz0BubCOEa5JkUAkbNmzAz8+PW2+91e7y\n/xz6D4npifxlyF9qOLKyK2/PQCoDIVyTJIMK0lqzfv167rjjDsxmc4nluZZc5u6ay8CggQxtM9QJ\nEZZNeXsGyVlSGQjhiiQZVNAvv/xCbGyswymi5b8s53zyeWYPmV3qWb3OVJ6egdZaKgMhXJQkgwpa\nv349gN1kYNVW3t31Lj0Ce3BnZ8fN5drAbDSXuTLIyssi15orRxMJ4YLkzKEK2rBhA71796ZFixYl\nlq09sZajl46y7J5lGFTtzrcmg6nMPQO5SJ0Qrqt2f1LVUteuXWP37t12DynVWvPOzndo36A9/9ft\n/5wQXfmUpzKQG9sI4bqkMqiAjRs3YrFY7E4RbTu3jX1x+/jozo/qxCUbTEYTVm3FYrVgNBhvuK5U\nBkK4LqkMKmDFihU0b96cAQMGlFj2zs53aObXjOm9pzshsvIzG/OPhCrLVJFcpE4I1yXJoJzS0tII\nCwtj4sSJGAzF//r2x+1ny5ktPDfgObw8vJwUYfmYDCaAMk0VSWUghOuSZFBO69evJysri/vuu6/E\nsnd2vkN9r/r8IfgPToisYgorgzIcXio3thHCdUkyKKcVK1bQrFkzBg8eXGz86KWjrDm+hj+G/BF/\nT38nRVd+JmP5KwNpIAvheiQZlEN6ejobNmzgnnvuwWgs3myds2sOPiYfnr7laSdFVzHl7Rl4GDzw\nMflUd1hCiBomyaAc1q9fT2ZmZokponNJ51j28zJm9p1JY5/GToquYsrbMwjwDKjVZ1QLISpGkkE5\nrFy5ksDAwBIXppu/ez4GZWDWoFlOiqziytszkH6BEK5JkkEZZWRksH79+hJTRInpiXz202dM6zWN\noHpBToywYsrbM5DDSoVwTZIMymj9+vVkZGSUmCJac2wNWXlZPHPLM06KrHLK1TOQykAIlyXJoIy+\n+OILmjdvztChxS9Hve7kOtrVb0f3wO5OiqxyytMzuJp5VZKBEC5KkkEZJCYmEhYWxtSpU/HwuH6J\niYzcDMLPhjO289g621QtT88gNiWWIP+6NxUmhCidJIMyWL58OXl5eUybNq3YePiZcLLyshjbZayT\nIqu8svYMkrOSSclOoXVA65oISwhRwyQZlMHnn39O37596d69+FTQupPr8Df71+o7mZWmrD2D6ORo\nAEkGQrgoSQal+OWXXzh48GCJqkBrzXcnvyO0Y2jhB2pdVNaegSQDIVybJINSfPHFF3h4ePDAAw8U\nGz8Yf5D4tHjGdq67U0RQ9p7B+eTzgCQDIVyVJIMbsFgsfPnll4wePZrAwMBiy9adXIdCMbrjaCdF\nVzXK2jOITo7GbDTT1K9pTYQlhKhhlU4GSqk/KqWOK6WOKKXmFhmfrZQ6pZQ6oZQKLTLeTykVZVv2\nvqrFh+GEh4dz4cIFpk8veW+C705+x8BWA2ni28QJkVWd8vQMWtVrVetv4ymEqJhK/c9WSg0DxgO9\ntNbdgPm28a7AJKAbMAr4UClVcNruIuBRoJPtZ1RlYqhOS5cupX79+tx1113Fxi+kXuBA/IE6P0UE\n5esZyBSREK6rsl/zHgfe1VpnA2itE23j44HlWutsrfVZ4BQQopRqDtTTWkdorTWwFJhQyRiqRWpq\nKqtXr2bSpEl4enoWW/b/ov4fAOO6jHNGaFWqrD0DSQZCuLbKJoPOwK1Kqb1KqR+UUv1t4y2BmCLr\nxdrGWtoe/3a81lm1ahWZmZkljiLKs+bxwb4PuK3tbXRt0tVJ0VWdsvQMci25xKXGSTIQwoWVesd2\npdQWoJmdRS/bXt8QGAD0B/6nlGpfVcEppWYCMwFat67ZD6KlS5fSsWPHEvc5/ub4N0QnR/P+qPdr\nNJ7qUpaewYXUC1i1VZKBEC6s1GSgtR7haJlS6nFgtW3KZ59Sygo0BuKAVkVWDbKNxdke/3bc0bY/\nAT4BCA4O1qXFWlWio6PZvn07r732WonLTLwX8R7tG7Tnrs53OXh13VKWnoGcYyCE66vsNNE3wDAA\npVRnwAxcBtYCk5RSnkqpduQ3ivdpreOBFKXUANtRRNOAbysZQ5VbtmwZWmumTp1abHx/3H52xezi\n6ZCnMRqMDl5dt3gY8r8P3KhnUJAM2gS0qZGYhBA1r9TKoBRLgCVKqV+AHGC6rUo4opT6H3AUyAOe\n1FpbbK95Avgv4A2E2X5qDa01S5cu5dZbb6V9++IzXgv3LsTf7M/DfR52UnRVTymFyWAqU2XQKqCV\nw3WEEHVbpZKB1joHmOpg2VvAW3bGI4Fae73nyMhIjh8/zqxZxe9adiH1Al8f+Zqn+j9FPc96Toqu\nepiN5hv2DKKTo2ns01jufSyEC5MziH7jP//5D56entx7773Fxpf8tIQ8ax5PhTzlpMiqj8l448rg\nfPJ56RcI4eIkGRSRlpbGl19+yf3330/9+tdv4qK1ZlnUMoa2GUqHhh2cGGH1MBvNpfYMJBkI4dok\nGRSxbNkyUlNT+cMf/lBs/KeEnzh++ThTekxxUmTV60Y9A60155PPS/NYCBcnycBGa82iRYvo1atX\niXMLlv28DJPBxL1d73Xw6rrtRj2D5Oxk0nLSpDIQwsVV9mgil7F3714OHz7MRx99VOzcAovVwle/\nfMWYTmNo6N3QiRFWH7PR7LAykHMMhHAPUhnYLFq0CH9/fyZPnlxsfPu57cSnxbvsFBHkN5AdVQbn\nk+Q+BkK4A0kGwJUrV/j666+ZOnUq/v7+xZZ9GfUl/mZ/lznj2B6pDIQQkgzIP5w0Ozubxx9/vNh4\nZm4mq46uYmLXiXibvJ0UXfUzGUwOjyYquKlNoG+g3eVCCNfg9skgLS2N+fPnc9ttt9GjR49iy9b/\nup7UnFSXniKCUiqDlPzDSuWmNkK4NrdvIC9cuJCLFy+yZs2aEsvWnlhLI+9GDGs7zAmR1ZwbnXQm\n5xgI4R7c+uvelStXmDt3LuPGjWPgwIHFllm1lY2nNhLaMdRlLkrniKPKQGvNmWtnJBkI4QbcOhnM\nmTOH1NRU3nqrxCWUOBh/kEsZl+r8De/LwtFJZzEpMSSkJRDcPNgJUQkhapLbJoO4uDg++OADpk6d\nSvfuJa+bF/ZrGApFaIdQJ0RXsxxdjmJ3zG4ABrUaVNMhCSFqmNsmg3feeQeLxcLrr79ud3nYqTCC\nWwTTxLdJDUdW8xz1DHbH7MbX5EuPpj3svEoI4UrcMhlYrVZWrFjB3XffTbt27Uosv5p5lb1xexnV\ncZQToqt5ji5HsStmF7cE3VJ4AxwhhOtyy2Swb98+EhMTGTdunN3lm09vxqqtbtEvAPs9g7ScNA4n\nHGZwq8FOikoIUZPcMhmsW7cOo9HI6NH2P+zDToXR0LshIS1Dajgy57DXM9gftx+Ltki/QAg34bbJ\nYPDgwTRsWPLCcwWHlI7sMNLlDyktYK8y2BWzC4ABQQPsvUQI4WLcLhmcP3+eqKgoh1NEhxIOcTH9\nottMEYH9nsHumN10a9KN+l71HbxKCOFK3C4ZrFu3DoCxY8faXf7t8W8B3OKQ0gK/PZrIqq3sid0j\n/QIh3IhbJoPOnTvTuXPnEsuSs5L5YN8HjOk0hqZ+TZ0QnXOYjWbyrHlorQE4fvk4SVlJ0i8Qwo24\nVTJITU1l+/btDquCf+75J9eyrvH3YX+v4cicy2QwARROFe2Kzu8XSDIQwn24VTLYtGkTOTk5dpPB\n5YzLLIhYwMSbJ9K3eV8nROc8ZqMZoPCIot2xu2ni04SODTs6MywhRA1yq2Swbt06GjRowODBJefC\n5+2aR1pOGq/fZv+MZFdmMuZXBgV9g90xuxnUalCx238KIVyb2yQDrTUbN24kNDQUD4/iZ9TGp8bz\nwb4PmNJzCt0CuzkpQucprAysuSRnJXPyykm3OcdCCJHPba4zEBUVxcWLFwkNLXmU0Nxdc8mx5PDq\n7151QmTOV9AzyLHk8OuVXwHo1bSXM0MSQtQwt6kMNm3aBMAdd9xRbDw5K5nFPy1mUvdJbjtHXrRn\nEJUYBSAXpxPCzbhVMujWrRstW7YsNr7kpyWk5aTxpwF/clJkzle0Z/DzxZ8J8AygVb1WTo5KCFGT\n3CIZZGZmsmPHDkaOHFls3GK18P6+9xnSegj9WvRzUnTOV7RnEJUYRY+mPaR5LISbcYtk8OOPP5Kd\nnV0iGaw9sZZzSed49pZnnRRZ7VDQM8jOyybqYhQ9AmWKSAh34xbJYNOmTZjNZoYOHVps/L2979Em\noA3jbxrvpMhqh4LK4My1MyRnJ9OzaU8nRySEqGlukwyGDBmCj49P4djB+IPsOL+DP4b80e1v3lLQ\nMzgQfwBAKgMh3JDLJ4P4+HiioqJKTBEtiFiAn9mPGX1nOCmy2qOgMihIBt0DS94TWgjh2lw+GWze\nvBmgWDI4l3SOr6K+YkafGQR4BTgrtFqjoGdw4MIB2gS0kb8TIdyQyyeDTZs20aRJE3r1un4S1bxd\n8zAoA7MGzXJiZLVHQWVwLeua9AuEcFMunQy01uzZs4c77rgDgyF/Vy+mXWTJoSVM6zWNoHpBTo6w\ndijoGYD0C4RwVy7dOVVKcfToUZKSkgrH3ot4j+y8bF4c/KITI6tdCioDQCoDIdyUS1cGAJ6enjRt\nmn+jmuSsZD6M/JB7u95L50Ylb27jrgp6BiCXoRDCXbl8Mijqw/0fkpKdwuwhs50dSq1SUBmYjWZJ\nkkK4qUolA6VUb6VUhFLqkFIqUikVUmTZbKXUKaXUCaVUaJHxfkqpKNuy91UNXffAqq0silzEHe3v\noE/zPjWxyTqjIBl0bdLV7c+5EMJdVbYymAu8rrXuDfzN9hylVFdgEtANGAV8qJQy2l6zCHgU6GT7\nGVXJGMrkx/M/EpMSw0O9H6qJzdUpBQ1k6RcI4b4qmww0UM/2OAC4YHs8Hliutc7WWp8FTgEhSqnm\nQD2tdYTOv/v6UmBCJWMok2VRy/A1+TK+i3tfesIeX5Mv9TzrMaTVEGeHIoRwksrOCTwLfK+Umk9+\nYim4g3pLIKLIerG2sVzb49+OV6vsvGxWHF3BhJsm4Gv2re7N1TmeHp6ce+acnGwmhBsrNRkopbYA\nzewsehkYDvxJa71KKfV/wGfAiKoKTik1E5gJ0Lp16wq/T9ipMJKykpjSY0pVheZyGng3cHYIQggn\nKjUZaK0dfrgrpZYCz9iergAW2x7HAUXvjhJkG4uzPf7tuKNtfwJ8AhAcHKxLi9WRZVHLaOLThDs6\n3FH6ykII4YYq2zO4APzO9vh24Ffb47XAJKWUp1KqHfmN4n1a63ggRSk1wHYU0TTg20rGcEPJWcms\nO7GO+7vdL0fKCCGEA5X9dHwUWKiU8gCysE3paK2PKKX+BxwF8oAntdYW22ueAP4LeANhtp9qs/rY\narIt2UzpKVNEQgjhSKWSgdZ6J2D3fpFa67eAt+yMRwI1do3kZVHL6NCgA7e0vKWmNimEEHWOS8+b\naK3p1bQXd3W+S+7pK4QQN+DSyUApxT9C/+HsMIQQotZzq2sTCSGEsE+SgRBCCEkGQgghJBkIIYRA\nkoEQQggkGQghhECSgRBCCCQZCCGEAFT+PWZqP6XUJeB8BV/eGLhcheHUBe64z+Ce++2O+wzuud8V\n2ec2Wusmpa1UZ5JBZSilIrXWwc6Ooya54z6De+63O+4zuOd+V+c+yzSREEIISQZCCCHcJxl84uwA\nnMAd9xncc7/dcZ/BPfe72vbZLXoGQgghbsxdKgMhhBA34NLJQCk1Sil1Qil1Sin1krPjqS5KqVZK\nqW1KqaNKqSNKqWds4w2VUpuVUr/a/mzg7FirmlLKqJT6SSn1ne25O+xzfaXUSqXUcaXUMaXUQFff\nb6XUn2y/278opb5SSnm54j4rpZYopRKVUr8UGXO4n0qp2bbPtxNKqdDKbNtlk4FSygj8GxgNdAUe\nUEp1dW5U1SYPmKW17goMAJ607etLQLjWuhMQbnvuap4BjhV57g77vBDYqLW+CehF/v677H4rpVoC\nTwPBWuvugBGYhGvu83+BUb8Zs7uftv/jk4Buttd8aPvcqxCXTQZACHBKa31Ga50DLAfGOzmmaqG1\njtdaH7Q9TiX/w6El+fv7uW21z4EJzomweiilgoA7gcVFhl19nwOAocBnAFrrHK11Ei6+3+TfldFb\nKeUB+AAXcMF91lrvAK7+ZtjRfo4Hlmuts7XWZ4FT5H/uVYgrJ4OWQEyR57G2MZemlGoL9AH2Ak21\n1vG2RQlAUyeFVV3eA14ErEXGXH2f2wGXgP/YpscWK6V8ceH91lrHAfOBaCAeSNZab8KF9/k3HO1n\nlX7GuXIycDtKKT9gFfCs1jql6DKdf9iYyxw6ppS6C0jUWh9wtI6r7bONB9AXWKS17gOk85vpEVfb\nb9sc+XjyE2ELwFcpNbXoOq62z45U5366cjKIA1oVeR5kG3NJSikT+YlgmdZ6tW34olKquW15cyDR\nWfFVg8HAOKXUOfKnAG9XSn2Ja+8z5H/7i9Va77U9X0l+cnDl/R4BnNVaX9Ja5wKrgUG49j4X5Wg/\nq/QzzpWTwX6gk1KqnVLKTH6jZa2TY6oWSilF/hzyMa31P4ssWgtMtz2eDnxb07FVF631bK11kNa6\nLfn/tlu11lNx4X0G0FonADFKqS62oeHAUVx7v6OBAUopH9vv+nDy+2KuvM9FOdrPtcAkpZSnUqod\n0AnYV+GtaK1d9gcYA5wETgMvOzueatzPIeSXjj8Dh2w/Y4BG5B998CuwBWjo7Firaf9vA76zPXb5\nfQZ6A5G2f+9vgAauvt/A68Bx4BfgC8DTFfcZ+Ir8vkgu+VXgIzfaT+Bl2+fbCWB0ZbYtZyALIYRw\n6WkiIYQQZSTJQAghhCQDIYQQkgyEEEIgyUAIIQSSDIQQQiDJQAghBJIMhBBCAP8feiqSCWSmT8UA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19fc3ddd390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(reward_list, color='k')\n",
    "plt.plot(threshold_list, color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[42mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | :\u001b[42m_\u001b[0m:G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: |\u001b[42m_\u001b[0m: :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "-1\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "20\n",
      "Done\n",
      "steps: 11, reward:9\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "reward_total = 0\n",
    "for i in range(1000):\n",
    "    env.render()\n",
    "    new_s, reward, done, _ = env.step(np.argmax(policy[s]))\n",
    "    s = new_s\n",
    "    reward_total = reward_total + reward\n",
    "    print(reward)\n",
    "    if done:\n",
    "        print('Done')\n",
    "        break\n",
    "print('steps: {0}, reward:{1}'.format(i, reward_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) crossentropy method\n",
    "\n",
    "In this section we will train a neural network policy for continuous action space game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #predict array of action probabilities\n",
    "        probs = agent.predict_proba([s])[0] \n",
    "        \n",
    "        a = <sample action with such probabilities>\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "percentile = 70\n",
    "smoothing = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    #generate new sessions\n",
    "    sessions = [generate_session() for _ in range(n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "\n",
    "    threshold = <select percentile of your samples>\n",
    "    \n",
    "    elite_states = <select states from sessions where rewards are above threshold>\n",
    "    elite_actions = <select actions from sessions where rewards are above threshold>\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #elite_states: a list of states from top games\n",
    "    #elite_actions: a list of actions from top games\n",
    "    \n",
    "    <fit agent to predict elite_actions(y) from elite_states(X)>\n",
    "\n",
    "\n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part I\n",
    "\n",
    "### Tabular correntropy method\n",
    "\n",
    "You may have noticed that the taxi problem quickly converges from -10k to aroung -500 score (+- 500) and stays there. This is in part because taxi-v2 has some hard-coded randomness in the environment. Other reason is that the percentile was chosen poorly.\n",
    "\n",
    "### Tasks\n",
    "- __1.1__ (1 pt) Modify the tabular CEM (CrossEntropyMethod) code to plot distribution of rewards and threshold on each tick.\n",
    "- __1.2__ (2 pts) Find out how the algorithm performance changes if you change different percentile and different n_samples.\n",
    "\n",
    "```<YOUR ANSWER>```\n",
    "\n",
    "\n",
    "- __1.3__ (2 pts) Tune the algorithm to end up with positive average score.\n",
    "- __1.4 bonus__ (1 pt) Try to achieve a distribution where 25% or more samples score above +9.0\n",
    "- __1.5 bonus__ (2 pts) Solve and upload [Taxi-v1](https://gym.openai.com/envs/Taxi-v1) to the openai gym.\n",
    "\n",
    "It's okay to modify the existing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part II\n",
    "\n",
    "### Deep crossentropy method\n",
    "\n",
    "By this moment you should have got enough score on [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) to consider it solved (see the link). It's time to upload the result and get to something harder.\n",
    "\n",
    "* if you have any trouble with CartPole-v0 and feel stuck, feel free to ask us or your peers for help.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* __2.1__ Go to the [gym site](http://gym.openai.com/), register and obtain __api key__.\n",
    "* __2.2__ (1 pt) Upload your result to gym via gym.upload (see Results tab above, the line you need is commented)\n",
    "* __2.3__ (3 pts) Pick one of environments: MountainCar-v0 or LunarLander-v2 (or both) and solve it.\n",
    "  * For MountainCar, learn to finish it in __less than 180 steps__\n",
    "  * For LunarLander, learn to get reward of __at least +50__\n",
    "  * See the tips section below, it's kinda important.\n",
    "  \n",
    "  \n",
    "* __2.4__ (1+ pt) Devise a way to speed up training at least 2x against the default version\n",
    "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8)\n",
    "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
    "  * Experiment with amount of training iterations and learning rate of the neural network (see params)\n",
    "  \n",
    "  \n",
    "### Tips\n",
    "* Gym page: [mountaincar](https://gym.openai.com/envs/MountainCar-v0), [lunarlander](https://gym.openai.com/envs/LunarLander-v2)\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ whule R > threshold works alright.\n",
    "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
    "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
    "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)\n",
    "* 20-neuron network is probably not enough, feel free to experiment.\n",
    "* __Please upload the results to openai gym and send links to all submissions in the e-mail__\n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "* __2.5 bonus__ Try to find a network architecture and training params that solve __both__ environments above (_Points depend on implementation_)\n",
    "\n",
    "* __2.6 bonus__ Solve continuous action space task with `MLPRegressor` or similar.\n",
    "  * [MountainCarContinuous-v0](https://gym.openai.com/envs/MountainCarContinuous-v0), [LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2) (4+ points if it works)\n",
    "  \n",
    "* __2.7 bonus__ Use any deep learning framework of your choice to implement policy-gradient (see lectures) on any of those envs (4 +1 per env):\n",
    "  * CartPole-v0\n",
    "  * MountainCar-v0\n",
    "  * LunarLander-v2\n",
    "  * See __tips on policy gradient__ below.\n",
    "  \n",
    "\n",
    "* __2.8 bonus__ take your favorite deep learning framework and try to get above random in [Atari Breakout](https://gym.openai.com/envs/Breakout-v0) with crossentropy method over a convolutional network.\n",
    "  * Expect at least +10 points if you get this up and running, no deadlines apply ! \n",
    "  * __See tips below on where to start, they're cruicially important__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips on policy gradient\n",
    "\n",
    "* The loss function is very similar to crossentropy method. You can get away with using rewards as  __sample_weights__.\n",
    "* If your algorithm converges to a poor strategy, try regularizing with entropy or just somehow prevent agent from picking actions deterministically (e.g. when probs = 0,0,1,0,0)\n",
    "* We will use `lasagne` later in the course so you can try to [learn it](http://lasagne.readthedocs.io/en/latest/user/tutorial.html).\n",
    "* If you don't want to mess with theano just yet, try [keras](https://keras.io/getting-started/sequential-model-guide/) or [mxnet](http://mxnet.io/tutorials/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tips on atari breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's all the pre-processing and tuning done for you in the code below\n",
    "* Once you got it working, it's probably a good idea to pre-train with autoencoder or something\n",
    "* We use last 4 frames as observations to account for ball velocity\n",
    "* The code below requires ```pip install Image``` and ```pip install gym[atari]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-25 22:38:13,932] Making new env: BreakoutDeterministic-v0\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[1;32mimport\u001b[0m \u001b[0matari_py\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'atari_py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-2c3a58742454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbreakout\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_breakout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_breakout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Documents\\My Projects\\Reinforcement Learning\\week1\\breakout.py\u001b[0m in \u001b[0;36mmake_breakout\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_breakout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[1;34m\"\"\"creates breakout env with all preprocessing done for you\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mPreprocessAtari\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"BreakoutDeterministic-v0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPreprocessAtari\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Making new env: %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestep_limit\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'vnc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_limit\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTimeLimit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempting to make deprecated env {}. (HINT: is there a newer registered version of this env?)'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_entry_point\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mentry_point\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEntryPoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x={}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentry_point\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, require, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2303\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrequire\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2304\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2305\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2307\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2309\u001b[0m         \u001b[0mResolve\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentry\u001b[0m \u001b[0mpoint\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mits\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \"\"\"\n\u001b[0;32m-> 2311\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__name__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\envs\\atari\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matari\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matari_env\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAtariEnv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0matari_py\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: No module named 'atari_py'. (HINT: you can install Atari dependencies by running 'pip install gym[atari]'.)"
     ]
    }
   ],
   "source": [
    "from breakout import make_breakout\n",
    "\n",
    "env = make_breakout()\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the initial state\n",
    "s = env.reset()\n",
    "print (s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plot first observation. Only one frame\n",
    "plt.imshow(s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#next frame\n",
    "new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#after 10 frames\n",
    "for _ in range(10):\n",
    "    new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "< tons of your code here or elsewhere >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
