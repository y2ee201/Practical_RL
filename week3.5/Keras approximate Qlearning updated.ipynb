{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-10 19:54:43,821] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "[ -2.18249957e+00  -1.52003326e+37   2.61521863e-01  -1.36268295e+37]\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.shape)\n",
    "print(env.observation_space.sample())\n",
    "print(env.action_space.n)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qlearner():\n",
    "    def __init__(self, epsilon, alpha, gamma):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(32, input_dim=4, activation='relu'))\n",
    "        self.model.add(Dense(32, activation='relu'))\n",
    "        self.model.add(Dense(2, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer = 'adam')\n",
    "    \n",
    "    def get_qvalue(self, state):\n",
    "        return self.model.predict(state)[0]\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if self.epsilon > np.random.rand():\n",
    "            return int(np.random.randint(low=0, high=2, size=1))\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "    \n",
    "    def update_model(self, state, next_state, reward, action):\n",
    "        q_curr = self.get_qvalue(state)\n",
    "        q_ref = reward + self.gamma * np.amax((self.get_qvalue(next_state)))\n",
    "        q_curr[action] = q_curr[action] + self.alpha * (q_ref - q_curr[action])\n",
    "        self.model.fit(x=state, y=q_curr.reshape(1, 2), epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent = Qlearner(epsilon=1, alpha=0.1, gamma=0.9)\n",
    "episodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00631537 -0.00217974]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "print(agent.get_qvalue(s.reshape((1,4))))\n",
    "print(agent.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sessions(t_max = 1000):\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        s = s.reshape((1, 4))\n",
    "        agent.get_qvalue(s)\n",
    "        a = agent.get_action(s)\n",
    "        \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        agent.update_model(action=a, next_state=next_s.reshape((1, 4)), state=s, reward=r)\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward = total_reward + r\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 mean reward:24.02\tepsilon:0.95\n",
      "episode 2 mean reward:22.72\tepsilon:0.9025\n",
      "episode 3 mean reward:23.39\tepsilon:0.8573749999999999\n",
      "episode 4 mean reward:20.12\tepsilon:0.8145062499999999\n",
      "episode 5 mean reward:19.81\tepsilon:0.7737809374999999\n",
      "episode 6 mean reward:19.77\tepsilon:0.7350918906249998\n",
      "episode 7 mean reward:18.24\tepsilon:0.6983372960937497\n",
      "episode 8 mean reward:19.75\tepsilon:0.6634204312890623\n",
      "episode 9 mean reward:18.44\tepsilon:0.6302494097246091\n",
      "episode 10 mean reward:14.17\tepsilon:0.5987369392383786\n",
      "episode 11 mean reward:16.68\tepsilon:0.5688000922764596\n",
      "episode 12 mean reward:13.48\tepsilon:0.5403600876626365\n",
      "episode 13 mean reward:14.03\tepsilon:0.5133420832795047\n",
      "episode 14 mean reward:14.36\tepsilon:0.48767497911552943\n",
      "episode 15 mean reward:12.93\tepsilon:0.46329123015975293\n",
      "episode 16 mean reward:12.97\tepsilon:0.44012666865176525\n",
      "episode 17 mean reward:13.18\tepsilon:0.41812033521917696\n",
      "episode 18 mean reward:13.53\tepsilon:0.3972143184582181\n",
      "episode 19 mean reward:12.03\tepsilon:0.37735360253530714\n",
      "episode 20 mean reward:12.07\tepsilon:0.35848592240854177\n",
      "episode 21 mean reward:12.48\tepsilon:0.34056162628811465\n",
      "episode 22 mean reward:11.7\tepsilon:0.3235335449737089\n",
      "episode 23 mean reward:11.43\tepsilon:0.30735686772502346\n",
      "episode 24 mean reward:11.12\tepsilon:0.2919890243387723\n",
      "episode 25 mean reward:11.19\tepsilon:0.27738957312183365\n",
      "episode 26 mean reward:11.4\tepsilon:0.263520094465742\n",
      "episode 27 mean reward:11.05\tepsilon:0.25034408974245487\n",
      "episode 28 mean reward:11.44\tepsilon:0.2378268852553321\n",
      "episode 29 mean reward:10.87\tepsilon:0.2259355409925655\n",
      "episode 30 mean reward:11.18\tepsilon:0.2146387639429372\n",
      "episode 31 mean reward:10.59\tepsilon:0.20390682574579033\n",
      "episode 32 mean reward:10.73\tepsilon:0.1937114844585008\n",
      "episode 33 mean reward:10.51\tepsilon:0.18402591023557577\n",
      "episode 34 mean reward:10.57\tepsilon:0.17482461472379698\n",
      "episode 35 mean reward:10.33\tepsilon:0.16608338398760714\n",
      "episode 36 mean reward:13.02\tepsilon:0.15777921478822676\n",
      "episode 37 mean reward:10.27\tepsilon:0.14989025404881542\n",
      "episode 38 mean reward:10.37\tepsilon:0.14239574134637464\n",
      "episode 39 mean reward:10.25\tepsilon:0.1352759542790559\n",
      "episode 40 mean reward:10.2\tepsilon:0.1285121565651031\n",
      "episode 41 mean reward:10.08\tepsilon:0.12208654873684793\n",
      "episode 42 mean reward:10.38\tepsilon:0.11598222130000553\n",
      "episode 43 mean reward:9.94\tepsilon:0.11018311023500525\n",
      "episode 44 mean reward:10.0\tepsilon:0.10467395472325498\n",
      "episode 45 mean reward:19.12\tepsilon:0.09944025698709223\n",
      "episode 46 mean reward:10.3\tepsilon:0.09446824413773762\n",
      "episode 47 mean reward:12.57\tepsilon:0.08974483193085074\n",
      "episode 48 mean reward:20.11\tepsilon:0.0852575903343082\n",
      "episode 49 mean reward:19.12\tepsilon:0.08099471081759278\n",
      "episode 50 mean reward:23.72\tepsilon:0.07694497527671314\n",
      "episode 51 mean reward:23.8\tepsilon:0.07309772651287748\n",
      "episode 52 mean reward:24.6\tepsilon:0.0694428401872336\n",
      "episode 53 mean reward:19.02\tepsilon:0.0659706981778719\n",
      "episode 54 mean reward:21.66\tepsilon:0.0626721632689783\n",
      "episode 55 mean reward:26.13\tepsilon:0.059538555105529384\n",
      "episode 56 mean reward:16.09\tepsilon:0.05656162735025291\n",
      "episode 57 mean reward:19.53\tepsilon:0.053733545982740265\n",
      "episode 58 mean reward:16.32\tepsilon:0.05104686868360325\n",
      "episode 59 mean reward:21.59\tepsilon:0.04849452524942309\n",
      "episode 60 mean reward:9.72\tepsilon:0.04606979898695193\n",
      "episode 61 mean reward:9.68\tepsilon:0.04376630903760433\n",
      "episode 62 mean reward:9.62\tepsilon:0.041577993585724116\n",
      "episode 63 mean reward:10.0\tepsilon:0.03949909390643791\n",
      "episode 64 mean reward:9.86\tepsilon:0.03752413921111601\n",
      "episode 65 mean reward:10.62\tepsilon:0.03564793225056021\n",
      "episode 66 mean reward:14.1\tepsilon:0.0338655356380322\n",
      "episode 67 mean reward:15.44\tepsilon:0.032172258856130585\n",
      "episode 68 mean reward:16.07\tepsilon:0.030563645913324056\n",
      "episode 69 mean reward:13.45\tepsilon:0.029035463617657853\n",
      "episode 70 mean reward:21.4\tepsilon:0.027583690436774957\n",
      "episode 71 mean reward:24.11\tepsilon:0.02620450591493621\n",
      "episode 72 mean reward:16.07\tepsilon:0.0248942806191894\n",
      "episode 73 mean reward:25.06\tepsilon:0.023649566588229927\n",
      "episode 74 mean reward:22.79\tepsilon:0.022467088258818428\n",
      "episode 75 mean reward:19.56\tepsilon:0.021343733845877507\n",
      "episode 76 mean reward:17.77\tepsilon:0.02027654715358363\n",
      "episode 77 mean reward:14.78\tepsilon:0.019262719795904448\n",
      "episode 78 mean reward:22.21\tepsilon:0.018299583806109226\n",
      "episode 79 mean reward:13.52\tepsilon:0.017384604615803764\n",
      "episode 80 mean reward:16.13\tepsilon:0.016515374385013576\n",
      "episode 81 mean reward:9.42\tepsilon:0.015689605665762895\n",
      "episode 82 mean reward:13.23\tepsilon:0.01490512538247475\n",
      "episode 83 mean reward:13.42\tepsilon:0.014159869113351011\n",
      "episode 84 mean reward:16.66\tepsilon:0.01345187565768346\n",
      "episode 85 mean reward:14.74\tepsilon:0.012779281874799287\n",
      "episode 86 mean reward:9.54\tepsilon:0.012140317781059323\n",
      "episode 87 mean reward:11.99\tepsilon:0.011533301892006355\n",
      "episode 88 mean reward:11.38\tepsilon:0.010956636797406038\n",
      "episode 89 mean reward:14.06\tepsilon:0.010408804957535735\n",
      "episode 90 mean reward:10.61\tepsilon:0.009888364709658948\n",
      "episode 91 mean reward:15.02\tepsilon:0.009888364709658948\n",
      "episode 92 mean reward:12.99\tepsilon:0.009888364709658948\n",
      "episode 93 mean reward:15.53\tepsilon:0.009888364709658948\n",
      "episode 94 mean reward:11.98\tepsilon:0.009888364709658948\n",
      "episode 95 mean reward:12.51\tepsilon:0.009888364709658948\n",
      "episode 96 mean reward:12.92\tepsilon:0.009888364709658948\n",
      "episode 97 mean reward:18.15\tepsilon:0.009888364709658948\n",
      "episode 98 mean reward:18.08\tepsilon:0.009888364709658948\n",
      "episode 99 mean reward:15.45\tepsilon:0.009888364709658948\n",
      "episode 100 mean reward:9.77\tepsilon:0.009888364709658948\n"
     ]
    }
   ],
   "source": [
    "for i in range(episodes):\n",
    "    rewards = [generate_sessions() for i in range(100)]\n",
    "    if agent.epsilon > 0.01:\n",
    "        agent.epsilon *= 0.95\n",
    "    print (\"episode {0} mean reward:{1}\\tepsilon:{2}\".format(i+1, np.mean(rewards),agent.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
